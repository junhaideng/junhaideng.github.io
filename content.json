{"pages":[{"title":"关于","text":"承蒙关注，受宠若惊。","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"images","text":"","link":"/images/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"一致性哈希","text":"背景在介绍一致性哈希之前，首先来看看集群部署可能发生的问题：比如说我现在有5台 Redis 服务器，正常运行了很久，很不巧有一天A服务器崩溃了，这个时候还有4台服务器，系统还可以正常运行，原来发送到A服务器的请求我们肯定要想办法进行重定向吧，如果说我们使用一般的哈希函数进行分配，无疑是 hash(key) % num，不过因为 num 现在变成了 num-1，那么很有可能所有的请求都会发生改变打到不同的服务器上，原来发送到B的请求重新处理之后可能发送到了C服务器了。 为了规避这种大迁移情况，我们可以使用一致性哈希。 算法原理一致性哈希算法使用的也是取模，但是不同于普通的哈希，我们不是对服务器的数量进行取模，而是对 2^32 取模，也就是相当于对一个固定的数进行取模，我们可以这样认为：所有的哈希值空间构成了一个圆环，如下图： 如果我们有多个服务器的，根据某个属性计算哈希值，比如说IP地址，映射到圆环上对应节点，假设说现在我有三个节点，映射之后的结果如下: 当某个请求发送过来的时候，根据其中的某个属性计算哈希值，比如说HTTP请求的路径，头部信息等，假设映射到如下的位置 根据映射到的位置，我们选择顺时针旋转遇到的第一个服务器节点作为该请求所选择的服务器节点，也就是 B 服务器。 当我们的某台服务器崩溃的时候，比如 B 崩溃了，那么 A 请求根据一致性哈希原理，会分配到服务器节点 C，更一般的，(A, B] 范围中的请求都会重新分配到服务器节点 C，而其他的请求都不会发生改变，这和之前所述的普通哈希函数不同。 但是上述的算法在某些情况下表现不好，比如说下面的这种服务器节点分布： C 一个节点就占用了大概2/3的取值空间，导致请求分配非常不均匀，针对这种情况，我们可以通过设置多个节点副本，如下： 这样处理之后，比之前而言要均匀了很多，理论来说，副本的数量越多，分配的越均匀，但是同时也会增加管理的难度，所以不宜设置过大。 算法实现先简单说一下思路吧，按照上面的原理，我们需要： hash 函数：能够支持多副本 服务器节点映射：哈希值-&gt;服务器节点 映射的值范围：类似上面的取值圆环1234567891011121314type consistent struct { // 副本数量 replicas int // 所有的server 节点 nodes map[string]struct{} // 节点所对应的server servers map[uint32]string // 保存所有的索引，也就是在hash圆环上的节点 circle uints // type uints []uint32 // 采用的hash算法 // hash 方法可能直接决定节点的分布情况 hash Hash sync.RWMutex} 当一个服务器节点进行映射的时候，会得到多个哈希值(多个副本)，这些哈希值在圆环上对应一个位置 但是我们没必要保存一个 2^31-1 大的数组，我们只需要维护一个有序数组就好了，当一个请求过来的时候，我们只需要获取到它的哈希值顺时针的下一个节点哈希值对应的服务器节点就好了 哈希函数哈希函数应该能够支持多副本，方式有很多种，这里采用一个简单的方式，也可以自定义 1234567891011import ( &quot;hash/fnv&quot; &quot;strconv&quot;)// replicafunc hash(key string, num int) uint32 { h := fnv.New32() h.Write([]byte(key + '-'+ strconv.Itoa(num))) return h.Sum32()} 服务器节点映射服务器节点进行哈希之后，需要保存对应的哈希值，这样可以通过哈希值获取到对应的服务器 1servers := make(map[uint32]string) 映射值的范围1circle := make([]uint32, 0) 每一次添加节点的时候，都需要进行重新排序： 1234567for i := 0; i &lt; replicas; i++ { key := hash(node, i) circle = append(c.circle, key) servers[key] = node}// 重新进行排序sort.Sort(c.circle) 而在删除节点的时候，我们只需要删除对应节点的哈希值，顺序维持不变即可。 支持的方法主要支持的方法不是很多，基本的有： 12345678type ConsistentHasher interface { // 添加节点 Add(slot string) // 删除节点 Delete(slot string) // 数据对应的节点 Get(key string) string} 添加服务器节点上面已经说了，删除节点的主要代码如下： 1234567891011121314151617181920212223242526func (c *consistent) Delete(node string) { c.Lock() defer c.Unlock() // 删除节点 delete(c.nodes, node) // 因为在数组中删除元素不方便，这里先记录一下需要删除的数据 // 然后如果在这里面的数据就不再添加到新的记录中 memo := make(map[uint32]struct{}) // 删除hash圆环中的值 for i := 0; i &lt; c.replicas; i++ { key := c.hashKey(node, i) memo[key] = struct{}{} delete(c.servers, key) } // 创建一个新的保存 newCircle := make(uints, 0, c.circle.Len()-c.replicas) for i := 0; i &lt; c.circle.Len(); i++ { if _, ok := memo[c.circle[i]]; !ok { newCircle = append(newCircle, c.circle[i]) } } c.circle = newCircle} 当我们需要获取一个请求对应的服务器节点的时候，我们只要搜索到顺序针第一个服务器节点即可，因为哈希值有序，这里可以采用二分查找的方式 123456789101112131415// Get 获取到属于的server节点func (c *consistent) Get(name string) string { c.RLock() defer c.RUnlock() // 首先将hash找到 key := c.hash(name) // 然后在Hash圆环上找到对应的节点 i := sort.Search(len(c.circle), func(i int) bool { return c.circle[i] &gt;= key }) // 顺时针计算应该就是第一个节点了 if i &gt;= c.circle.Len() { i = 0 } return c.servers[c.circle[i]]} 具体的实现代码已经上传到 GitHub 上了 总结一致性哈希在加入和删除节点的时候只会影响相邻的节点，对其他的节点无影响。当节点数量发生变化，我们不希望映射关系全部被打乱的时候，可以采用该算法。","link":"/2021/11/09/alg/consistent-hash/"},{"title":"负载均衡","text":"背景在实际生产过程中，我们往往会通过集群的方式部署服务器，而不是单机部署，从而可以提高服务的并发能力。 但是这样部署产生了一个新的问题：如何决定某个请求发往的服务器？这就是负载均衡算法所需要解决的问题。 算法目前比较常见的负载均衡算法有： 轮询法 (Round Robin) 随机法 (Random) 权重轮询 权重随机 哈希法 最小响应时间 最小连接 下面一一进行介绍 注意：为了简单，下面的代码未考虑并发，仅提供算法思路 轮询法轮询法属于最简单的负载均衡策略之一，它的意思是说，这一次请求选择的是 A 服务器，那么下一次请求我们就选择 A 后面的服务器，依次类推，到达最后一个位置之后，重新选择第一个服务器。 1234567891011121314func RoundRobin(servers []string) func() string { // 保存访问位置 pos := 0 return func() string { res := servers[pos] // 移动一个位置，下一次选择这个 pos++ // 如果达到最后一个 if pos == len(servers){ pos = 0 } return res }} 随机法通过生成随机数，从而选择对应的服务器，算法十分简单 123456789func init(){ // 随机数种子初始化 rand.Seed(time.Now().UnixNano())}func Random(servers []string) string{ index := rand.Intn(len(servers)) return servers[index]} 权重轮询在部署服务的时候，有些服务器可能性能好一点，有些服务器呢，性能差一点，这样的话，如果所有服务器平摊全部请求，肯定不好，那么我们可以给每一个服务器设置一个权重，权重越大，表示性能越好，可以接受更多的服务。比如说现在有两台服务器，A 服务器权重为 3，B 服务器权重为 1，那么平均下来，A 服务器应该接受 3/4 的请求，而B服务器接受 1/4 的请求。 一种很简单的思路就是将所有的服务器按照权重平铺延展，类似下图，然后使用轮询的方式 1234567891011121314151617181920212223type Server struct { ip string weight int}func WeigthRoundRobin(servers []Server) func() string { pos := 0 s := make([]string, 0, len(servers)) for i := 0; i &lt; len(servers); i++ { for j := 0; j &lt; servers[i].weight; j++ { s = append(s, servers[i].ip) } } return func() string { res := s[pos] pos++ if pos == len(s) { pos = 0 } return res }} 但是这种算法有所缺陷，那就是不平滑，在一个时间段中可能只有 A 服务器在用，但是 B 服务器可能都没有接收到任何请求，针对这种情况，出现了一种比较平滑的算法，nginx 中的权重轮询就是该类。该算法的思想是： 服务器的初始权重(current)为0，有效权重(effective)为设置的权重值，计算所有的权重和为 total 每一轮选择中，将当前的权重+有效权重，current 最大的为本次选中的服务器 选中该服务器之后，current 需要减去 total 进行下一轮 下面是权重为 3 的服务器 A 和权重为 1 的服务器 B 执行该算法的一整轮过程 请求次数 current 添加 effective 后 total 选中 选中后 current 1 [0, 0] [3, 1] 4 A [-1, 1] 2 [-1, 1] [2, 2] 4 A [-2, 2] 3 [-2, 2] [1, 3] 4 B [1, -1] 4 [1, -1] [4, 0] 4 A [0, 0] 代码实现如下： 123456789101112131415161718192021222324func SmoothWeightRoundRobin(servers []Server) func() string { current := make([]int, len(servers)) total := 0 for i := 0; i &lt; len(servers); i++ { total += servers[i].weight } return func() string { max := 0 index := 0 // 加入 effective 权重 // 并且找到 current 权重最大的 for i := 0; i &lt; len(servers); i++ { current[i] += servers[i].weight if current[i] &gt; max { max = current[i] index = i } } current[index] -= total return servers[index].ip }} 权重随机权重随机可以类比权重轮询的第一种实现方式，下面直接给出代码 12345678910111213141516func WeigthRandom(servers []Server) func() string { rand.Seed(time.Now().UnixNano()) s := make([]string, 0, len(servers)) for i := 0; i &lt; len(servers); i++ { for j := 0; j &lt; servers[i].weight; j++ { s = append(s, servers[i].ip) } } return func() string { index := rand.Intn(len(s)) return s[index] }} 除此之外还可以使用前缀和的思路，将所有的权重构成一个前缀和数组，比如说权重为[1, 2,3] 的时候，可以形成 [0, 1, 3, 6]，轮询的时候随机生成一个索引([0,6)之间)，然后到数组中查找对应的范围，比如随机出 5，首先 +1，属于 (3, 6]，所以应该返回最后一个节点，也就是说返回大于等于 随机数+1 的前缀和数组值对应的节点，具体的算法实现见 GitHub[^1] 哈希法我们可以对请求中的某个特定属性计算哈希值，然后根据哈希值寻找对应的服务器节点。 12345678910func HashLoadBalance(servers []string) func(key string) string { return func(key string) string { f := fnv.New32() f.Write([]byte(key)) // 计算 hash 值 h := f.Sum32() // 取模获取对应的服务器 return servers[int(h)%len(servers)] }} 也可以使用一致性哈希算法 最小响应时间负载均衡器需要统计服务端响应的时间，然后选择平均响应时间最小的服务器。 12345678910111213141516171819202122232425262728293031// 响应时间统计type RT struct { count int // 多少次 total time.Duration // 总时间}type LoadBalancer struct { servers []string rt map[string]RT // 响应时间}func (l *LoadBalancer) LeastResponseTime() string { if len(l.servers) == 0 { return &quot;&quot; } // 初始化 server := l.servers[0] min := l.rt[server].total / time.Duration(l.rt[server].count) // 选择最小的 for i := 1; i &lt; len(l.servers); i++ { rt := l.rt[l.servers[i]] average := rt.total / time.Duration(rt.count) if average &lt; min { min = average server = l.servers[i] } } return server} 最小连接数和最小响应时间类似，需要负载均衡器维护每一个服务器的连接，然后选择连接数量最少的服务器。 总结负载均衡算法是集群部署时常用的算法，合理的负载均衡算法能够更好程度的利用我们的服务器资源，提高系统的承受能力，具体的算法需要根据我们的需求进行选择，比如提供 WebSocket 服务的时候，应该使用 哈希法，根据客户端的 IP 地址计算访问的服务端，从而保证先后访问的服务端都是同一个服务端。 实现代码参见 GitHub","link":"/2021/11/10/alg/load-balance/"},{"title":"限流算法","text":"常见的限流算法主要有两种：令牌桶和漏桶算法，也可以使用计数器进行粗暴限流实现。 算法原理计数器维护一个请求数量，在一段时间里，如果请求总数超过了limit，那么我们可以把这个请求拒绝掉，也可以将其放入到缓冲队列中，等待下一个时间段再进行操作。 例如在上图中，如果时间1~2中出现了110个请求，那么后面的10个请求就会被拒绝掉，如果这一个时间段中的请求数量在100之内，那么每一个请求都会进行响应。 漏桶算法计数器算法中容易出现毛刺，比如说在60s中我们允许100个请求，但是前1s的时候我们就接收到了100个请求，那么后面的59s我们不会对任何的请求进行响应。 在漏桶算法中我们将请求依次添加到桶中间，添加的时候如果桶子已经满了，那么我们将这些请求给丢掉或者缓存，另一端以一定速率对请求进行消费。 一般来说，漏桶是一个FIFO的队列，我们消耗的往往都是最先发起的那一个请求。 漏桶算法可以使得输出变得平滑，因为算法中请求以固定速率进行消费，也正因为如此，所以漏桶算法不支持突发的请求。 令牌桶算法令牌桶算法是漏桶算法的一种改进，我们设置一个令牌漏斗，桶中初始化一定数量的令牌了，然后按照一定的速率往漏斗中添加令牌，每一个请求到来的时候都会往漏桶中获取需要数量的令牌，如果令牌不够，那么拒绝该请求或者缓存。 相较漏桶算法而言，令牌桶算法允许一定程度的突发。 算法实现限流算法需要实现的基本功能就是判断是否能够放行该请求，如果不行的话，给一个需要的等待的时长，决定是否等待该时长，或者丢弃该请求。 12345type RateLimiter interface { // 返回的值表示需要等待的时间 // 达到这个时间之后才可能获取到 Take(num int) time.Duration} 下面代码仅为展现实现思路，并未考虑并发问题 计数器算法维护一个计数器 count，一段时间后清零。 123456789101112131415161718192021222324// 计数器type counter struct { // 当前请求数 count int lastTime time.Time // 表示1s中限制允许通过的数量 limit int}func (c *counter) Take(num int) time.Duration { // 每隔 1s 清理一次 now := time.Now() if now.Sub(c.lastTime) &gt; time.Second { c.count = 0 } c.lastTime = now // 如果该段时间中还允许 num量级 的请求 if c.count+num &lt;= c.limit { c.count += num return 0 } // 可能需要等待的时间 return time.Duration(c.count+num-c.limit) * time.Second} 漏桶算法漏桶算法保证请求匀速进行消费，我们需要维护漏桶的容量，已经放入的请求数量，上一次进行消耗请求的时间。当有一个请求到来的时候，首先根据当前的时间和上一次消耗请求的时间，计算这段时间里已经消费了多少的请求，然后添加该请求进去，如果没有超过最大容量，那么允许该请求，如果超过了容量，那么被放弃或者等待。 12345678910111213141516171819202122232425262728type bucket struct { // 桶的容量 capacity int64 lastTime time.Time // 桶中请求的数量 count int64 // 限制数，最终规整化为 1 duration 中多少个 limit int64}func (b *bucket) Take(num int64) time.Duration { // 首先计算当前的时间差 diff := time.Now().Sub(b.lastTime) // 可以消费的数量 count := b.limit * int64(diff) // 消耗之后还有多少请求在桶中 b.count -= count // 如果桶中还可以放入这么多的 if b.count+num &lt; b.capacity { return 0 } // 一次性获取 num 失败，需要再等待多少时间重试 return time.Duration((b.count + num - b.capacity) / b.limit)} 令牌桶算法漏桶算法和令牌桶算法实现的时候比较相似，但是漏桶算法每一轮结算的时候计算消费的请求，减少容器里面的请求数量，而令牌桶算法每轮结算的时候计算产生的令牌数，然后添加到容器中。 123456789101112131415161718192021222324252627282930type token struct { // 令牌桶容量 capacity int64 lastTime time.Time // 令牌桶中的令牌数量 count int64 // 每个 duration 允许通过的数量 limit int64}func (t *token) Take(num int64) time.Duration { diff := time.Now().Sub(t.lastTime) // 时间差中可以生成多少的令牌 count := t.limit * int64(diff) // 加入令牌 t.count += count // 满了的令牌抛弃 if t.count &gt; t.capacity { t.count = t.capacity } // 需要的令牌少于桶中的令牌 if num &lt;= t.count { return 0 } // 需要等待多少时间 return time.Duration((num - t.count) / t.limit)} 总结当系统的 QPS 太高扛不住了，我们可以使用限流算法限制住一部分的流量，比如对于 HTTP 请求可以返回 429 给用户，同时如果说我们只想要用户在一个时间段里面最多请求 n 次，也可以使用限流算法。 在工程运用中，实现限流算法的时候通常会采用无锁操作，这样可以带来更高的效率。 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156package mainimport ( &quot;fmt&quot; &quot;time&quot;)// 本代码仅为思路讲解，并未考虑并发问题// 工程运用中一般会考虑无锁操作// 有兴趣的同学可以参考 https://github.com/uber-go/ratelimittype RateLimiter interface { // 返回的值表示需要等待的时间 // 达到这个时间之后才能获取到 Take(num int) time.Duration}// 计数器type counter struct { // 当前请求数 count int lastTime time.Time // 表示1s中限制允许通过的数量 limit int}func (c *counter) Take(num int) time.Duration { // 每隔 1s 清理一次 now := time.Now() if now.Sub(c.lastTime) &gt; time.Second { c.count = 0 } c.lastTime = now // 如果该段时间中还允许 num量级 的请求 if c.count+num &lt;= c.limit { c.count += num return 0 } // 可能需要等待的时间 // 为什么可能？因为需要考虑并发 return time.Duration(c.count+num-c.limit) * time.Second}// 漏桶算法type bucket struct { // 桶的容量 capacity int64 lastTime time.Time // 桶中请求的数量 count int64 // 限制数，最终规整化为 1 duration 中多少个 limit int64}func (b *bucket) Take(num int64) time.Duration { // 首先计算当前的时间差 diff := time.Now().Sub(b.lastTime) // 可以消费的数量 count := b.limit * int64(diff) // 消耗之后还有多少请求在桶中 b.count -= count // 如果桶中还可以放入这么多的 if b.count+num &lt; b.capacity { return 0 } // 一次性获取 num 失败，需要再等待多少时间重试 return time.Duration((b.count + num - b.capacity) / b.limit)}// 令牌桶算法type token struct { // 令牌桶容量 capacity int64 lastTime time.Time // 令牌桶中的令牌数量 count int64 // 每个 duration 允许通过的数量 limit int64}func (t *token) Take(num int64) time.Duration { diff := time.Now().Sub(t.lastTime) // 时间差中可以生成多少的令牌 count := t.limit * int64(diff) // 加入令牌 t.count += count // 满了的令牌抛弃 if t.count &gt; t.capacity { t.count = t.capacity } // 需要的令牌少于桶中的令牌 if num &lt;= t.count { return 0 } // 需要等待多少时间 return time.Duration((num - t.count) / t.limit)}func testCounter() { rt := &amp;counter{ 0, time.Now(), 1, } // 有一个请求会被block for i := 0; i &lt; 10; i++ { fmt.Println(rt.Take(1)) fmt.Println(rt.Take(1)) time.Sleep(time.Second) } rt.limit = 2 // 都可以通过 for i := 0; i &lt; 10; i++ { fmt.Println(rt.Take(1)) fmt.Println(rt.Take(1)) time.Sleep(time.Second) }}func testBucket() { rt := &amp;bucket{ 1, time.Now(), 0, 1, } for i := 0; i &lt; 10; i++ { fmt.Println(rt.Take(time.Second.Nanoseconds())) fmt.Println(rt.Take(100)) // time.Sleep(time.Second) }}func testToken() { rt := &amp;token{ 10, time.Now(), 0, 1, } for i := 0; i &lt; 10; i++ { fmt.Println(rt.Take(time.Second.Nanoseconds())) fmt.Println(rt.Take(1)) // time.Sleep(time.Second) }}func main() { testCounter() // testBucket() // testToken()}","link":"/2021/11/14/alg/rate-limiter/"},{"title":"Snowflake ID 算法","text":"背景假设我们有一个分布式系统，系统中需要维护全局 id 字段，我们可以把它认为是唯一的标识，不能够重复出现，那么问题来了，我们应该如何生成这样的 id 呢？ 其实很容易想到的一种解决方式就是使用 Redis 的键值对了，每次更新的时候直接调用 incr，生成的 id 也是唯一的，还有一种方式就是使用 MySQL 或者其他的数据库，因为我们知道 MySQL 中可以生成自增主键，使用这个主键作为一个分布式 id 也是可行的。 但是上面的这两种方式效率不会特别高，并且依赖于第三方，我们如果想要更高效的生成分布式 id，那么最好的方式就是尽量本地生成，不需要和其他节点进行协商，但是有一个问题出现了，如何保证 id 不重复？，我们可以使用 Snowflake 算法来解决该问题。 概念Snowflake 可用于在分布式系统中生成唯一的 id，由 Twitter 提出，目前存在很多不同的版本，但是基本的思想是一致的，只不过不同版本不同结构采用的位数不一致。 Snowflakes 使用 64 比特， 但是只有 63 位被使用，第一个比特位为符号位，大体结构如下： timestamp 占 41 bits，是生成 ID 的时间戳，也可以是相对某一个特定时间的时间戳差，machine id 为分布式系统每一个机器分配到的 id 号，10 bits 表示最多 1024 台机器，sequence number 表示序列号，因为同一个时间戳可能分配多个 id。 ID 生成每一台机器的 machine id 都是事先配置好的，可以由数据中心 id 和数据中心的机器 id 组成，直接可以获取到。当我们需要生成一个 id 的时候，首先我们需要获取当前的时间戳，判断是否和上一次的时间戳一致，如果说和上一次的时间戳一致，那么我们应该增加序列号，然后通过移位操作构造对应的一个 64 bits 的 id 号返回。 如果说当前的时间戳与上一次的不同，那么我们直接修改时间戳，然后序列号取零，进行拼接即可。 代码实现算法的实现挺简单的，下面给出核心代码： 12345678910111213141516171819202122func (s *Snowflake) GetID() int64 { // 首先获取当前的时间戳 timestamp := time.Now().UnixMilli() // 相同的时间戳序列号+1 if timestamp == s.LastTimestamp { s.Sequence = (s.Sequence + 1) &amp; sequenceMask // 重新绕了一圈 // 同一个时间戳里面生成了很多 id if s.Sequence == 0 { for timestamp &lt;= s.LastTimestamp { timestamp = time.Now().UnixMilli() } } } else { s.Sequence = 0 } // 重新设置 s.LastTimestamp = timestamp // 进行拼接 return (timestamp-epoch)&lt;&lt;timestampShift | (s.DatacenterID &lt;&lt; datacenterIDShift) | (s.WorkerID &lt;&lt; workerIDShift) | s.Sequence} 测试写了一个很简单的基准测试： 123456func BenchmarkGetID(b *testing.B) { s := New(10, 10) for i := 0; i &lt; b.N; i++ { s.GetID() }} 下面是它的结果，表现还不错吧，这样算起来，假设每次需要 250 ns，那么 1s 也可以生成 4,000,000 个不同的 id 。 1234567goos: windowsgoarch: amd64pkg: snowflakecpu: Intel(R) Core(TM) i7-8565U CPU @ 1.80GHzBenchmarkGetID-8 4897538 246.0 ns/opPASSok snowflake 2.069s Twitter 其实也提供了自己的 Scala 实现方式，具体的可以参见 GitHub， 本文的实现方式可以参见 我的仓库。","link":"/2021/11/24/alg/snowflake/"},{"title":"八大排序算法及其代码实现","text":"选择排序原理每次选择数组中的最小元素，排在第一个位置 算法步骤12345678[38,65,97,76,13,27,49]13 [65 97 76 38 27 49]13 27 [97 76 38 65 49]13 27 38 [76 97 65 49]13 27 38 49 [97 65 76]13 27 38 49 65 [97 76]13 27 38 49 65 76 [97]13 27 38 49 65 76 97 代码实现1234567891011121314151617181920212223func SelectSort(data []int) { length := len(data) for i := 0; i &lt; length; i++ { // 记录第一个数据 min := data[i] index := i // 获取到最小的值以及对应的索引 for j := i + 1; j &lt; length; j++ { if min &gt; data[j] { min = data[j] index = j } } // 如果最小的值不是当前序列的第一个值 // 那么需要进行交换 if index != i { // 交换最小值 data[index], data[i] = data[i], min } }} 插入排序原理假设之前的数组都是一个有序序列，其余的记录为无序序列，从这些无序序列中不断选取数据，插入到前面已经排序好的有序序列中 算法步骤1234567[38] 65 97 76 13 27 49[38 65] 97 76 13 27 49[38 65 97] 76 13 27 49[38 65 76 97] 13 27 49[13 38 65 76 97] 27 49[13 27 38 65 76 97] 49[13 27 38 49 65 76 97] 代码实现1234567891011121314151617func InsertSort(data []int) { for i := 1; i &lt; len(data); i++ { tmp, j := data[i], i // 如果第j-1个元素比第i个元素大 // 不满足递增的条件，第i个需要移动到前面 if data[j-1] &gt; tmp { // 将第i个元素移动到前面的适当位置 // 也就是将元素不断的向右边移动，直到找到合适的位置摆放 for j &gt;= 1 &amp;&amp; data[j-1] &gt; tmp { data[j] = data[j-1] j-- } } // 将当前遍历到的值填充到对应的位置 data[j] = tmp }} 冒泡排序原理基本思路: 从第一个元素开始一次对相邻的记录进行比较， 当前面的记录大于后面的记录，交换其位置，进行一轮比较和换位之后，n个记录中的最大记录将位于第n位 算法步骤12345678{38 65 97 76 13 27 49}38 65 76 13 27 49 [97]38 65 13 27 49 [76 97]38 13 27 49 [65 76 97]13 27 38 [49 65 76 97]13 27 [38 49 65 76 97]13 [27 38 49 65 76 97][13 27 38 49 65 76 97] 代码实现12345678910111213141516171819func BubbleSort(nums []int) { flag := false n := len(nums) // 这里采用的是不断将最大的移动到数组的最后面 // 当然也可以将最小的移动到最前面来 for i := 0; i &lt; n ; i ++{ for j := 0; j &lt; n-1 -i; j ++{ if nums[j] &gt; nums[j+1]{ nums[j], nums[j+1] = nums[j+1], nums[j] flag = true } } if !flag{ fmt.Println(&quot;一轮遍历中没有进行交换，说明数组已经有序&quot;) break } fmt.Println(nums) }} 归并排序原理将一个数组拆成两份，每一份进行递归操作之后都是有序的，然后进行合并，最终是有序的。 算法步骤1234567{38 65 97 76 13 27 49}[38 65 97 76] [13 27 49] [38 65] [97 76] [13 27] [49][38] [65] [97] [76] [13] [27] [49] // 全部分裂[38 65] [76 97] [13 27] [49] // 开始进行合并[38 65 76 97] [13 27 49][13 27 38 49 65 76 97] 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647func MergerSort(nums []int) { mergeSort(nums, 0, len(nums)-1)}func mergeSort(nums []int, left, right int) { if left &gt;= right { return } mid := (right-left)/2 + left mergeSort(nums, left, mid) mergeSort(nums, mid+1, right) merge(nums, left, mid, right)}// 合并两个数组，分别是从 left -&gt; mid , mid -&gt; right 索引开始func merge(nums []int, left, mid, right int) { // 辅助数据 tmp := make([]int, right-left+1) for i := left; i &lt;= right; i++ { tmp[i-left] = nums[i] } i, j := 0, mid-left+1 k := left for k &lt;= right &amp;&amp; i &lt;= mid-left &amp;&amp; j &lt;= right-left { if tmp[i] &gt; tmp[j] { nums[k] = tmp[j] k++ j++ } else { nums[k] = tmp[i] i++ k++ } } for i &lt;= mid-left { nums[k] = tmp[i] i++ k++ } for j &lt;= right-left { nums[k] = tmp[j] j++ k++ }} 快速排序快速排序是一种非常高效的排序算法，它采用分而治之的思想，把大的拆分成小的，小的再拆分成更小的。 原理对于一组给定的记录，通过一趟排序之后，将原来的序列分成两部分，其中一部分的所有记录均比后一部分的所有记录小，然后再依次对前后两部分的记录进行快速排序，递归改过程，直到序列中的所有记录均有序为止 可以直接认为是找到了一个分界点，比如如果需要实现递增，那么左边都是小于该数，右边都是大于该数 算法步骤 分解：将输入的序列array[m...n] 划分成两个非空子序列array[m...k]和array[k+1...n]，使得array[k+1...n]中的任意一个元素都不小于array[m...k]中的元素 递归求解：通过递归调用快速排序算法分别对array[m...k]和array[k+1...n]进行排序 合并：由于对分解出来的两个子序列的排序是就地进行的，所以在array[m...k]和array[k+1...n]都排好序后不需要执行任何计算array[m...n] 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758func sort(arr []int, left, right int) { if left &gt;= right { return } i := left j := right // 选定一个分界点 // 如果直接下面这种方式取的话，如果数据是有序的，那么时间复杂度相比较会比较高 index := arr[i] // 也可以随机挑选一个 // rand.Seed(time.Now().UnixNano()) // var random int // random = rand.Int() % (right-left+1) + left // 将随机选中的移动到最左边 // arr[i], arr[random] = arr[random], arr[i] // 这里只是为了更新上面的index，如果直接写的话，可以去掉上的赋值，直接使用index:= arr[i] // index = arr[i] for i &lt; j { // 右边界不断向左移动，如果碰到比index小的数据 // 则需要将左边的i对应的数据赋值为对应的值 for i &lt; j &amp;&amp; arr[j] &gt;= index { j-- } if i &lt; j { arr[i] = arr[j] i++ } // 左边界不断往右移动，如果碰到比index小的数据 // 则将右边的这个数据赋值索引j对应的数据 for i &lt; j &amp;&amp; arr[i] &lt;= index { i++ } if i &lt; j { arr[j] = arr[i] j-- } } // 索引i表示的就是 arr[i] = index // 排序左边子数组 sort(arr, left, i-1) // 排序右边 sort(arr, i+1, right)}func QuickSort(arr []int) { sort(arr, 0, len(arr)-1)}​ 基准关键字的选取常用的基准关键字的选取有以下方式: 选取首尾，中间位置上的中值作为基准关键字 选取随机数 希尔排序原理类似插入排序，但是不是相邻的进行，而是会跳过几个位置 算法步骤1234567891011{38, 65, 97, 76, 13, 27, 49}step为: 3 [38 65 97 76 13 27 49] // 索引 3 和 0 进行比较并交换step为: 3 [38 13 97 76 65 27 49] // 索引 4 和 1 进行比较并交换step为: 3 [38 13 27 76 65 97 49] // 索引 5 和 2 进行比较并交换step为: 3 [38 13 27 49 65 97 76] // 索引 6 和 3 进行比较并交换step为: 1 [13 38 27 49 65 97 76] // 索引 1 和 0 进行比较并交换step为: 1 [13 27 38 49 65 97 76] // 索引 2 和 1 进行比较并交换step为: 1 [13 27 38 49 65 97 76] // 索引 3 和 2 进行比较并交换step为: 1 [13 27 38 49 65 97 76] // 索引 4 和 3 进行比较并交换step为: 1 [13 27 38 49 65 97 76] // 索引 5 和 4 进行比较并交换step为: 1 [13 27 38 49 65 76 97] // 索引 6 和 5 进行比较并交换 代码实现1234567891011121314151617func ShellSort(nums []int) { // 步长 for step := len(nums) / 2; step &gt; 0; step /= 2 { for i := step ; i &lt; len(nums); i++ { // 直接插入排序 if nums[i] &lt; nums[i-step] { j, tmp := i, nums[i] // 找出同一组中比tmp小的值，往后面移动step个位置 for j &gt;= step &amp;&amp; tmp &lt; nums[j-step] { nums[j] = nums[j-step] j -= step } nums[j] = tmp } } }} 堆排序原理这里说一下递增排序，使用大根堆 大根堆有一个特点，那就是堆的顶部是整个数组中最大的元素，一个比较简单的想法就是，我把这个元素拿出来，然后插入到其他的数组中，堆中 pop 出堆顶的元素，依次进行，便可以获取到一个排序序列，不过这种思路需要多余的一个数组，其实没有必要，我们来自己看看删除元素的过程。 首先，将堆顶的元素和最后一个元素进行交换，然后 size--，进行向下冒泡 down，删除掉最后一个元素。 1234567891011func pop(arr *[]int) int { nums := *arr n := len(nums) - 1 // 交换 nums[0], nums[n] = nums[n], nums[0] down(nums, 0, n) x := nums[n] // 删除最后一个元素 *arr = (*arr)[:n] return x} 注意这里删除最后一个元素，我们可以把这个元素保留在这里，但是让 down 的时候不接触到这个元素不就行了？这个很好实现，因为一般我们都是使用元素的长度作为 down 的一个 size 参数，我们指定一下就好，主要的逻辑代码如下： 123456789101112func heapSort(nums []int) { size := len(nums) // 首先建堆 buildHeap(nums) // 然后这个时候最大的元素在第一个 for size &gt; 1 { // 第一个元素和最后一个进行交换 nums[0], nums[size-1] = nums[size-1], nums[0] size-- down(nums, 0, size) }} 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596// 建大根堆func buildHeap(nums []int) { size := len(nums) for i := size / 2; i &gt;= 0; i-- { down(nums, i, size) }}// 插入元素的时候需要使用func up(nums []int, i int) { for { // 父结点计算公式！！ parent := (i - 1) / 2 // 如果父结点大于子节点了，说明不需要调换，已经满足条件了 // 如果 i == parent 说明 i = 0 了，没有父结点，也不需要调换 if i == parent || nums[parent] &gt; nums[i] { break } // 父结点没有子节点i的值大，需要调换 nums[parent], nums[i] = nums[i], nums[parent] }}// 初始化的时候需要建堆// 删除元素的时候需要down：// 将第一个元素和最后一个元素进行调换, 然后重新 down(nums, 0, size)func down(nums []int, i int, size int) { left, right := 2*i+1, 2*i+2 j := i // 找到最大的元素的下标 if left &lt; size &amp;&amp; nums[left] &gt; nums[j] { j = left } if right &lt; size &amp;&amp; nums[right] &gt; nums[j] { j = right } if j != i { nums[i], nums[j] = nums[j], nums[i] down(nums, j, size) }}// 不用递归也可以func down2(nums []int, i, size int) { for { left, right := i*2+1, i*2+2 // 说明i是叶子节点 if left &gt;= size { break } // 找到最大的节点 largest := i if left &lt; size &amp;&amp; nums[left] &gt; nums[largest] { largest = left } if right &lt; size &amp;&amp; nums[right] &gt; nums[largest] { largest = right } if largest != i { nums[i], nums[largest] = nums[largest], nums[i] i = largest } else { // 如果最大的节点就是本身，那么后面也不需要进行操作了 break } }}// 从堆中弹出一个数// 首先将堆顶的元素和最后一个元素进行调换，然后进行down操作func pop(arr *[]int) int { nums := *arr n := len(nums) - 1 nums[0], nums[n] = nums[n], nums[0] down(nums, 0, n) x := nums[n] *arr = (*arr)[:n] return x}func HeapSort(nums []int) { size := len(nums) // 首先建堆 buildHeap(nums) // 然后这个时候最大的元素在第一个 for size &gt; 1 { // 第一个元素和最后一个进行交换 nums[0], nums[size-1] = nums[size-1], nums[0] size-- down(nums, 0, size) }} 计数排序原理设置一个长度为元素最大值最小值的空间，将元素一个一个对应一个索引，如果这个索引中已经存在元素，那么对应的值+1，说明这个索引处对应多个元素，最后遍历一遍这个索引空间，值为0的不需要添加，值为多个的一次添加到原数组中 计数排序适合于数组中的值在一定范围内的数据，比如说年龄，数据可能需要进行整理，比如 [10000, 20000] 的数据，我们可以首先整理为 [0, 10000]，然后再转换回去。 代码实现123456789101112131415// 比如我们需要排序年龄，那么我们可以估算年龄在[0,200]中func CountingSort(nums []int) { count := make([]int, 200) for _, num := range nums { count[num] ++ } index := 0 for i :=0; i &lt; len(count); i ++ { // 出现了多少次，排序后添加到原数组多少次 for j:= 0; j &lt; count[i]; j++ { nums[index] = i index ++ } }} 总结下表给出了每种排序算法的稳定性和效率的比较： 排序方法 最好时间 平均时间 最坏时间 辅助储存 稳定性 备注 简单选择排序 $$O(n^2)$$ $$O(n^2)$$ $$O(n^2)$$ $$O(1)$$ 不稳定 n小时较好 直接插入排序 $$O(n)$$ $$O(n^2)$$ $$O(n^2)$$ $$O(1)$$ 稳定 大部分有序时较好 冒泡排序 $$O(n)$$ $$O(n^2)$$ $$O(n^2)$$ $$O(1)$$ 稳定 n小时较好 希尔排序 $$O(n)$$ $$O(n\\log n)$$ $$O(sn)\\ 1&lt;s&lt;2$$ $$O(1)$$ 不稳定 s是所选分组 快速排序 $$O(n\\log n)$$ $$O(n\\log n)$$ $$O(n^2)$$ $$O(\\log n)$$ 不稳定 n大时较好 堆排序 $$O(n\\log n)$$ $$O(n\\log n)$$ $$O(n\\log n)$$ $$O(1)$$ 不稳定 n大时较好 归并排序 $$O(n\\log n)$$ $$O(n\\log n)$$ $$O(n\\log n)$$ $$O(n)$$ 稳定 n大时较好 如果想要更直观的看排序过程，推荐使用 Data Structure Visualizations 这个网站进行理解。","link":"/2021/11/29/alg/sort/"},{"title":"如何搭建个人博客？","text":"前言每一个技术同学都希望能够拥有一份自己的博客，有时候在上面写写东西，不需要太多，不需要炫彩夺目，简简单单就好，如果自己的文章能够帮助到他人那就更加欢喜不得了。 第一次看到 CSDN 上的写的一篇文章如此受大家的喜欢，甚是喜悦，后来又陆陆续续写了很多文章，不论是写代码时遇到的 bug 解决思路，还是刚学习某门语言时碰到的问题，亦或是学的某些算法等等。 收到 GitHub 上许多同学的点赞之后，也越来越喜欢开源自己写的 Project，同时 GitHub 上有着许许多多的有些仓库值得我们去学习。 在 CSDN 和 掘金 上写了这么些文章，但总是觉点欠缺点什么，前不久看到其他同学的博客，于是想着搭建自己的一份博客，一来可以方便内容的聚集，二来也算是满足自己的一个小小的愿望，谁不想要有自己的博客呢😄 选材网上其实有很多的博客模板，我们也没有必要从零开始搭建一个博客，当然，如果有时间有能力的话，也未尝不可是一次很好的尝试呢，因为在尝试的过程中，你可能会遇到很多问题，而这些问题恰恰就是你学习的机会。 在这里，我选择的是 Hexo 博客模板再加上 Icarus 主题，选择 Hexo 博客模板是因为其支持 Markdown 编写文章，并且生态很好，对于主题的挑选随自己的心愿，可以到主题模块去探索符合自己口味的。 刚开始选择的主题是 NexT，长下面这样，其实还可以，自定义之后也可以做的很好看。 后来看到了 Icarus 主题，经过配置之后达到下面的效果，这才是我想要的主题嘛，简洁大方，界面UI也做的很不错。 步骤下面假设已经预先安装好了 Node.js 和 Git，若尚未安装好，可以进入相应的官网进行下载安装，过程比较简单，这里不再赘述。 安装脚手架Hexo 提供了脚手架，我们可以方便的创建博客，首先进行安装： 1$ npm install -g hexo-cli 安装完成之后，如果执行 hexo -v 可以返回对应的版本号，则安装成功。 123456789101112131415161718$ hexo -vhexo-cli: 4.3.0os: win32 10.0.22000node: 14.17.4v8: 8.4.371.23-node.76uv: 1.41.0zlib: 1.2.11brotli: 1.0.9ares: 1.17.1modules: 83nghttp2: 1.42.0napi: 8llhttp: 2.1.3openssl: 1.1.1kcldr: 39.0icu: 69.1tz: 2021aunicode: 13.0 创建博客首先执行 hexo init [name] 初始化博客。 12345$ hexo init blogINFO Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitINFO Install dependencies...INFO Start blogging with Hexo! 博客项目初始化完成之后，我们就已经搭建了一个很简单的博客网站了，运行 hexo server 或者 hexo s 命令，运行博客： 1234$ hexo sINFO Validating configINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 样式如下，依个人看法，十分丑陋😂 编写文章使用 hexo new [title] 即可创建一篇文章，使用 markdown 进行编写即可，创建的文件在 source/_posts/test.md。 如果需要引入图片的话，注意这里不能直接引入本地图片，可以首先创建一个界面，然后将图片放置在该目录中： 首先创建 images 界面：hexo new page images，其实相当于多了二级路由 /images 将需要添加的图片放在 images 目录下，可以在其中再创建目录 使用绝对路径引用图片：比如 img 标签的 src 应为 /images/test.png，i/mages/blog/test.png 当然如果说想要直接引用本地文件，可以参考 asset-folders，首先将配置文件 _config.yml 中的 post_asset_folder 设置为 true，创建新文章时自动创建一个文件夹，这个资源文件夹将会有与这个文章文件一样的名字，需要的资源放在该目录中，即可通过相对路径访问了。举个栗子，文章 test.md 对应的test 目录下放了一个 go.png，那么在 test.md 中可以使用 &lt;img src=&quot;go.png&quot;&gt; 进行引用。 美化美化，当然要看主题了，Hexo 主要提供的是一个框架，我们可以任意选择主题，当然也可以自制主题，只不过成本比较高，下面以 Icarus 主题为例，演示如何设置主题。 首先下载对应的主题文件，这里推荐使用 git clone。 1git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus 然后在 _config.yml 中配置主题为 icarus，或者使用命令 hexo config theme icarus 进行设置，然后重启即可，效果如下： 过程中可能有些模块没有安装，使用 npm install 或者 yarn add 添加即可。 部署这里使用 GitHub Pages 进行部署，不需要自己购买服务器，当然如果想部署在自己的服务器也是可以的，推荐使用 Nginx 搭建，下面进行演示。 在 GitHub 创建一个仓库，命名为 &lt;your name&gt;.github.io，比如用户名为xiaoming 的同学应该命名为xiaoming.github.io。 然后在 _config.yml 中进行配置，这里注意需要把本地的 ssh 公钥添加到 GitHub 中。 12345678# 配置 urlurl: https://junhaideng.github.io/# 配置部署的仓库deploy: type: 'git' repo: 'git@github.com:junhaideng/junhaideng.github.io.git' branch: 'main' 执行 npm run deploy，项目即可部署，其实就是相当于把打包后的文件通过 Git 上传到仓库中。进入代码仓库的 Settings &gt; Pages，可以看到项目已经成功部署。 其他插件Icarus 中支持丰富的插件和个性化配置，比如 百度统计、Latex公式、布局样式等等，同时也支持评论，使用 Gitalk 十分简单的就可以搭建一个评论系统，这些就留给读者慢慢探索了，其官方文档中也有详细说明。","link":"/2021/12/21/blog/deploy/"},{"title":"Docker 基本使用","text":"Docker 是一个现代化的容器技术，使用它我们可以轻松的将应用进行打包，然后移植到其他系统，不必要在不同设备上反复配置对应的环境。 下面介绍一下基本使用： 从一个Hello world开始Docker 可以允许你在容器中运行应用程序，使用 docker run 可以在容器中运行一个应用程序。 1docker run ubuntu:18.04 /bin/echo &quot;Hello world&quot; run: 表示运行一个容器 ubuntu:18.04: 指定要运行的镜像，Docker 首先在本地主机上查找对应的镜像是否存在，如果不存在的话，那么会从远程镜像仓库 Docker Hub 中下载公共镜像，如果没有指定后面的 TAG，那么默认情况下使用的是 latest /bin/echo &quot;Hello world&quot;: 在启动的容器中执行的命令 运行完上面的命令之后，我们会下载一个 ubuntu 镜像，并且创建一个容器，使用下面的命令可以查看所有的镜像文件 1docker images 如果我们不需要这些容器了，我们可以使用下面命令进行删除 12# container 指的是容器的ID或者NAME，可以一次性删除多个docker rm container [container...] 运行交互式容器12docker run -i -t ubuntu:18.04 /bin/bash# -i -t 可以合并为-it -t: 在新容器中指令一个伪终端或终端 -i: 允许对容器中的标准输入(stdin)进行交互 输入exit可以退出当前容器 运行后台模式容器使用下面的命令可以创建一个以进程方式运行的容器 1docker run -d ubuntu:18.04 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot; 运行完上面的命令之后会输出一串字符串，表示的是这个 容器ID 容器使用获取镜像如果本地没有镜像，那么可以使用下面的命令获取 ubuntu 镜像 1docker pull ubuntu 启动容器使用下面命令启动一个容器，并且以命令行模式进入该容器 12docker run -it ubuntu /bin/bash# 默认使用的就是 /bin/bash 使用 exit 可退出终端 后台运行在大部分的场景下，我们希望 docker 的服务是在后台运行的，使用 -d 参数指定容器以 守护进程 的形式运行 1docker run -itd --name ubuntu-test ubuntu /bin/bash 启动、停止容器12345docker stop container# 启动一个已经停止的容器docker start container# 重启一个容器docker restart container 进入容器在使用 -d 参数时，容器启动会进入后台，此时如果想要进入容器，可以使用下面的指令进入 docker attach container docker exec container 推荐使用 docker exec，因为使用这个方式退出终端之后不会导致容器的停止 使用 docker exec 的时候需要指定运行的命令，比如 1docker exec -it 243c32535da7 /bin/bash 而使用 docker attach 的时候会直接进行上一次的命令 删除容器1docker rm container 删除容器的时候必须是停止状态 查看网络端口映射12345# PORTS信息中有docker ps# 查看指定容器的端口映射docker port container 查看容器日志1docker logs container 查看容器的进程1docker top container 检查容器输出容器的一系列信息 1docker inspect container Docker 镜像使用当运行容器的时候，如果使用的容器本地不存在，那么 docker 会自动从镜像仓库中下载，默认是 Docker Hub 公共镜像源 列出所有镜像列表1docker images 获取一个新的镜像1docker pull ubuntu:18.04 查找镜像1docker search xxx 删除镜像镜像的删除使用 docker rmi 命令 1docker rmi imageName 设置镜像标签1docker tag container xxx:xxx Docker 容器网络连接容器中可以运行一些网络应用，要想让外部也可以访问里面的这些应用，那么可以通过 -P 或者 -p 参数来指定端口映射 网络端口映射比如说运行一个 python 应用 123docker run -d -P webapp python app.py# 将容器内容的5000(2)端口映射到宿主机的5000(1)端口docker run -d -p 5000:5000 webapp python app.py -P: 容器里面的端口随机映射到主机端口 -p: 容器内部端口绑定到指定的主机端口 端口进行映射之后，我们可以使用 docker ps 或者 docker port container 进行查看对应的端口映射关系 默认情况下绑定的都是 tcp 端口，如果要绑定 udp 端口，可以再端口后面加 /udp 1docker run -d -p 127.0.0.1:5000:5000/udp webapp python app.py Docker 容器互联端口映射不是唯一的将 docker 连接到另一个容器的方法docker 有一个连接系统允许将多个容器连接在一起，共享连接信息docker 连接会创建一个父子关系，其中父容器可以看到子容器的信息 端口命名当我们创建一个容器的时候，docker 会自动对它进行命名，我们可以使用 --name 参数来自定义命名 123docker run -d --name ubuntu -t ubuntu:18.04 /bin/bash# 下面方法可以修改一个container的namedocker rename container NEW_NAME 新建网络1docker network eg: 12docker network create -d bridge test-net# -d 表示新建网络内省，有bridge, overlay 连接容器运行一个容器并连接到新建的网络中 1docker run -itd --name test1 --network test-net ubuntu /bin/bash 然后在打开一个新的终端运行一个容器并加入到 test-net 网络中 1docker run -itd --name test2 --network test-net ubuntu /bin/bash 然后可以在容器之间进行 ping 12ping test1 # test2内ping test2 # test1内 DockerfileDockerfile 可以用来构建一个镜像，文本内容包含了一条条构建镜像所需要的指令和说明 使用 Dockerfile 定制镜像1234567FROM nodeCOPY . /serverWORKDIR /serverRUN npm install --productionEXPOSE 3001ENTRYPOINT [ &quot;npm&quot;, &quot;run&quot;, &quot;start&quot;] FROM 指定使用哪一个镜像源RUN 指定要执行的命令COPY 执行相应的复制操作 1docker build -t centos:6.7 DockerfilePath -t: 指定创建的目标镜像名 DockerfilePath: 指定需要使用的 Dockerfile 文件，也可以指定所在目录，但是这样目录中的构建文件名必须是 Dockerfile 构建镜像在 Dockerfile 文件的存放目录下，执行构建动作 1docker build -t nginx:v3 . ., 上下文路径， 可以用 Dockerfile 的路径来代替，也可以使用目录，只不过构建文件名必须为 Dockerfile 指令详解COPY复制指令，从上下文目录中复制文件或者目录到容器里指定路径。 格式： 12COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径1&gt;... &lt;目标路径&gt;COPY [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;源路径1&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;] --chown=：可选参数，用户改变复制到容器内文件的拥有者和属组。 源路径：源文件或者源目录，这里可以是通配符表达式，其通配符规则要满足 Go 的 filepath.Match 规则。例如： 12COPY hom* /mydir/COPY hom?.txt /mydir/ &lt;目标路径&gt;：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。 ADDADD 指令和 COPY 的使用格式一致（同样需求下，官方推荐使用 COPY）。功能也类似，不同之处如下： ADD 的优点：在执行 &lt;源文件&gt; 为 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，会自动复制并解压到 &lt;目标路径&gt;。 ADD 的缺点：在不解压的前提下，无法复制 tar 压缩文件。会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。具体是否使用，可以根据是否需要自动解压来决定。 CMD类似于 RUN 指令，用于运行程序，但二者运行的时间点不同: CMD 在 docker run 时运行。 RUN 是在docker build。 123CMD &lt;shell 命令&gt;CMD [&quot;&lt;可执行文件或命令&gt;&quot;,&quot;&lt;param1&gt;&quot;,&quot;&lt;param2&gt;&quot;,...]CMD [&quot;&lt;param1&gt;&quot;,&quot;&lt;param2&gt;&quot;,...] # 该写法是为 ENTRYPOINT 指令指定的程序提供默认参数 推荐使用第二种格式，执行过程比较明确。第一种格式实际上在运行的过程中也会自动转换成第二种格式运行，并且默认可执行文件是 sh。 ENV设置环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。 格式： 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... EXPOSE声明端口 作用： 帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。 在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 格式： 1EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] WORKDIR指定工作目录。用 WORKDIR 指定的工作目录，会在构建镜像的每一层中都存在。（WORKDIR 指定的工作目录，必须是提前创建好的）。 docker build 构建镜像过程中的，每一个 RUN 命令都是新建的一层。只有通过 WORKDIR 创建的目录才会一直存在。 格式： 1WORKDIR &lt;工作目录路径&gt; USER用于指定执行后续命令的用户和用户组，这边只是切换后续命令执行的用户（用户和用户组必须提前已经存在）。 格式： 1USER &lt;用户名&gt;[:&lt;用户组&gt;]","link":"/2021/07/10/docker/basic/"},{"title":"使用 dlv 进行 debug","text":"使用 dlv 可以方便的对 go 生成的二进制文件进行 debug，以下面的代码 main.go 为例讲解 dlv 的使用 123456789101112131415161718192021package mainimport ( &quot;fmt&quot; &quot;net/http&quot; &quot;time&quot;)func Hello(w http.ResponseWriter, r *http.Request) { q := r.URL.Query().Get(&quot;q&quot;) if q != &quot;&quot; { fmt.Printf(&quot;search query: %s\\n&quot;, q) } fmt.Fprintf(w, &quot;Hello World! %s&quot;, time.Now())}func main() { http.HandleFunc(&quot;/&quot;, Hello) http.ListenAndServe(&quot;:8080&quot;, nil)} 1️⃣ 安装 dlv直接使用 go install 命令即可将 dlv 安装到 $GOPATH/bin 目录下面 12345# 可以下载最新的go install github.com/go-delve/delve/cmd/dlv@latest# 也可以下载指定的版本go install github.com/go-delve/delve/cmd/dlv@v1.7.3 2️⃣ 编译代码，设置编译参数直接使用 go build 编译二进制文件的时候，会对代码进行优化和内联，导致使用 dlv 的时候部分命令不能使用，比如函数的调用。 12345# 存在优化的编译go build -o http_server main.go# 去掉编译优化go build -gcflags=&quot;all=-N -l&quot; -o http_server main.go 3️⃣ dlv exec path/to/bin，进入调试模式使用 exec 子命令，直接调试可运行的二进制文件 123➜ dlv exec ./http_serverType 'help' for list of commands.(dlv) 4️⃣ 比对文件路径和行数，设置对应的断点比如我想在 main.go 的 Hello 函数进行调试，定位这个函数的行数为第 9 行，路径为 main.go，然后可以执行 break main.go:9 12(dlv) break main.go:9Breakpoint 1 set at 0x6e1bef for main.Hello() ./main.go:9 如果提示 location xxx ambiguous，需要对路径进行明确，使用输出的长路径即可，比如 break github.com/xxx/xxx/main.go。 设置好断点之后，可以通过 breakpoints命令查询所有的断点，可以发现 dlv 默认也会将 panic 加入断点。 12345(dlv) breakpointsBreakpoint runtime-fatal-throw (enabled) at 0x438720 for runtime.throw() /usr/local/go/src/runtime/panic.go:1188 (0)Breakpoint unrecovered-panic (enabled) at 0x438a80 for runtime.fatalpanic() /usr/local/go/src/runtime/panic.go:1271 (0) print runtime.curg._panic.argBreakpoint 1 (enabled) at 0x6e1bef for main.Hello() ./main.go:9 (1) 5️⃣ 执行程序，直到运行到断点或者程序结束这一步不能省略！！！ 省略之后无法调试断点，执行 continue 命令 123(dlv) continue# or(dlv) c 6️⃣ 进行请求，使其能够执行到断点处如果请求 ok 了，continue 命令不再阻塞 12345678910111213(dlv) continue&gt; main.Hello() ./main.go:9 (hits goroutine(18):1 total:2) (PC: 0x6e1bef) 4: &quot;fmt&quot; 5: &quot;net/http&quot; 6: &quot;time&quot; 7: ) 8:=&gt; 9: func Hello(w http.ResponseWriter, r *http.Request) { 10: q := r.URL.Query().Get(&quot;q&quot;) 11: if q != &quot;&quot; { 12: fmt.Printf(&quot;search query: %s\\n&quot;, q) 13: } 14: fmt.Fprintf(w, &quot;Hello World! %s&quot;, time.Now() 如果出现 warning 是因为二进制文件是经过优化的，编译的时候可以添加 -gcflags=&quot;all=-N -l&quot; 参数去掉该提示。 7️⃣ 愉快的进行调试使用 dlv 提供的命令，进行调试 命令 说明 args 当前函数的参数 step 进入函数/方法中 next 跳转到下一行代码 continue/c 运行代码直到下一个断点或者结束 break/b 设置断点 breakpoints/bp 所有的断点 clear xx 清理掉第 xx 个断点 clearall 清理所有的断点 locals 输出所有局部变量 print 输出指定的变量 print 如果需要输出长字符串，可以先执行 config max-string-len 1000 扩大输出的字符串长度 编译时需要禁止优化才能使用 1go build xxxx -gcflags=&quot;all=-N -l&quot; 命令 说明 list 输出源代码 call 调用函数 8️⃣ Moredlv 不仅使用 attach 对正在运行的程序进行调试，也可以直接对二进制文件进行调试 (dlv exec)，甚至是没有编译的代码包 (dlv debug)，更多更详细的文档，可以参考下面的两个链接: dlv 使用手册 dlv 命令文档 多动手试试，很容易就可以熟悉 dlv 使用~","link":"/2022/11/16/golang/dlv/"},{"title":"Golang 选项模式","text":"需求Golang不支持在函数定义的时候声明默认值，也就是说我们不能够使用类似下面的代码 123func New(addr string=&quot;localhost&quot;, port string=&quot;8080&quot;){ ...} 并且也不支持函数的重载，函数名不能够重复，无法根据需要的参数数量创建不同的函数形式，也就是我们不能使用下面的方式 1234567func New(addr string){ ...}// 再次使用同样的函数名会报错func New(addr string, port string){ ...} 返回默认对象那么我们如何才能设置默认值呢？一种很自然的方式比如说，我们创建对象的时候就返回一个带有默认值的对象 123456789101112type myStruct struct { Host string Port string}func New() *myStruct{ // 返回一个带有默认值的对象 return &amp;myStruct{ Host: &quot;0.0.0.0&quot;, Port: &quot;8080&quot;, }} 然后我们如果需要修改参数，那么我们可以手动去修改 123config := New()// 手动修改config.Port = &quot;8000&quot; 上面的这种方式其实已经可以解决我们的需求，但是还是不够好，因为这种方式会直接暴露结构体中的字段，结构体中的字段名首字母必须大写，并且用户进行修改的时候会直接对字段进行修改，无法对设置的参数进行校验。 Option模式下面我们主要讲解 Option模式，也就是 函数选项模式，这种模式下在我们设置默认值的同时支持检验参数。 Option 模式为 Golang 的开发者提供了将一个函数的参数设置为可选的功能，也就是说我们可以选择参数中的某几个，并且可以按任意顺序传入参数。 下面我们来看看具体如何使用： 首先，我们需要设置一个结构体包括我们需要设置的一些参数 1234type Config struct{ Addr string Host string} 然后，我们需要定义一个Option函数类型可以修改我们的参数结构体 1type Option func(c *Config) 对于我们需要进行修改的参数，我们可以声明对应的函数来修改参数值，这里利用了闭包的特性。 1234567891011func WithHost(host string) Option { return func(c *Config) { c.Host = host }}func WithPort(port string) Option { return func(c *Config) { c.Port = port }} 接着，我们需要设置一个默认值，如下，也就是用户没有设置任何参数的时候所得到的值 1234var defaultConfig = Config{ Host: &quot;0.0.0.0&quot;, Port: &quot;8080&quot;,} 最后，提供一个函数接口，创建对象的时候，用户可以执行一系列的Option来修改默认的参数 12345678910111213141516171819type myStruct struct { host string port string}func New(opts ...Option) *myStruct { // 默认情况下使用默认参数 config := defaultConfig for _, opt := range opts { // opt 就是一个函数，以 *Config为参数，能够修改其中的内容 opt(&amp;config) } return &amp;myStruct{ host: config.Host, port: config.Port, }} 这里还有另外一种方式实现，思路差不多，直接上代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849type myStruct struct { host string port string}// 设置一个接口类型type Option interface { apply(*myStruct)}// 声明一个类型，以需要修改的结构体为参数type optinFunc func(*myStruct)// 实现Option接口func (f optinFunc) apply(m *myStruct) { f(m)}// 修改参数func WithHost(host string) Option { return optinFunc(func(ms *myStruct) { ms.host = host })}func WithPort(port string) Option { return optinFunc(func(ms *myStruct) { ms.port = port })}var defaultOne = myStruct{ host: &quot;0.0.0.0&quot;, port: &quot;8080&quot;,}// 创建一个新的对象func New(options ...Option) *myStruct { m := defaultOne for _, opt := range options { opt.apply(&amp;m) } return &amp;m}// 可以认为我们创建的WithHost, WithPort// 就是一个可以修改struct字段的函数// 一般我们会暴露这些函数给使用// 但是在包内部我们可以修改结构体的私有成员 测试结果 1234567fmt.Printf(&quot;%#v\\n&quot;, New())fmt.Printf(&quot;%#v\\n&quot;, New(WithHost(&quot;127.0.0.1&quot;)))fmt.Printf(&quot;%#v\\n&quot;, New(WithHost(&quot;127.0.0.1&quot;), WithPort(&quot;8000&quot;)))// 输出// &amp;main.myStruct{host:&quot;0.0.0.0&quot;, port:&quot;8080&quot;}// &amp;main.myStruct{host:&quot;127.0.0.1&quot;, port:&quot;8080&quot;}// &amp;main.myStruct{host:&quot;127.0.0.1&quot;, port:&quot;8000&quot;} 虽然说，Option模式在设置参数的时候特别有用，而且显得十分优雅，但是通过对它的实现，我们也知道使用起来相对而言还是比较麻烦，需要添加很多函数","link":"/2021/07/13/golang/options/"},{"title":"动手写一个 HTTP 正向代理","text":"概念首先我们来了解一下 HTTP 代理的相关概念，通常来说，有两类 HTTP 代理，一类是正向代理，一类是反向代理，虽都是代理，但仍有区别。 我们平时使用的 VPN 就是正向代理，我们指定一个服务器，然后通过正向代理去连接这个服务器获取资源 Nginx 是典型的反向代理服务器，可以用于负载均衡和缓存，我们不清楚服务器的地址，但是我们访问反向代理服务器的时候，它会自动为我们请求服务器，并且返回相应的内容。 总结：正向代理下，我们知道服务器的具体地址，反向代理下，我们只需要知道代理服务的地址，无需知道具体的服务端地址。 这里，我们讲述正向代理，首先先来简单的回忆一下 HTTP 协议吧，也可以参考之前的 文章 HTTP 协议HTTP 基于传输层协议而搭建的应用层协议，其实 HTTP 请求和响应没有什么很神奇的部分，就是一个 Client/Server 模型，客户端通过套接字发送数据，服务端解析之后进行处理，然后返回响应。 这里只稍微介绍一下协议格式，想要了解更多的同学可以参考 HTTP/1.1，下面的内容均通过 wireshark 抓包获得。 请求格式如下： 每一行均以 \\r\\n 结尾，请求体可以为空 响应格式如下： 每一行也是以 \\r\\n 结尾，响应体可以为空 请求和响应在格式上的主要区别在第一行，也就是请求行和响应行的内容不一致，此外，请求和响应的头部取值也有所区别，部分头部只用于请求，部分头部只由于响应，但是常见的绝大多数头部在响应和请求中均可使用。 我们如果要构造一个请求也很简单，创建一个套接字，然后发送上述格式的数据即可： 123456789101112131415161718192021// 连接服务器conn, err := net.Dial(&quot;tcp&quot;, &quot;httpbin.org:80&quot;)if err != nil { fmt.Println(&quot;Dial tcp err: &quot;, err) return }// 构造请求msg := strings.Builder{}msg.WriteString(&quot;GET /get HTTP/1.1\\r\\n&quot;)msg.WriteString(&quot;Host: httpbin.org\\r\\n&quot;)msg.WriteString(&quot;Accept: application/json\\r\\n&quot;)msg.WriteString(&quot;Connection: close\\r\\n&quot;)msg.WriteString(&quot;\\r\\n&quot;)// 发送内容_, err = conn.Write([]byte(msg.String()))if err != nil { fmt.Println(&quot;Send msg err: &quot;, err) return } 同理，响应也是，这里不再赘述，读者可自行编写代码，也可以使用 wireshark 抓包测试。 正向代理实现上面提到过，在正向代理下，我们会首先连接到代理服务器，然后代理服务会到服务器上请求对应的资源，那么我们身为代理服务器，如何知道客户端需要请求什么资源呢？ 这到不同担心，客户端连接代理的时候，是有一定的规范的，不是说随便连，对于 HTTP 协议来说，请求代理服务器和请求普通的服务器大致相同，但是请求的路径一般会设置为绝对路径，比如为 GET http://httpbin.org/ HTTP/1.1，而不是 GET / HTTP/1.1，对于 HTTPS 来说，首先会通过 CONNECT 连接到代理服务器，接受到 200 响应之后才会发送实际的加密数据。 首先我们来考虑一下 HTTP 吧，下面是连接到服务器上的请求： 我们需要做的是获取到对应的服务器地址，也就是 Host 字段，将头部数据解析完成之后，获取该字段即可。 123456789101112131415161718192021222324252627282930313233343536373839404142type Request struct { Method string Path string Version string Headers http.Header Body []byte raw []byte // 原始请求}func (r Request) Host() (string, bool) { if r.Headers.Get(&quot;Host&quot;) != &quot;&quot; { return r.Headers.Get(&quot;Host&quot;), true } return &quot;&quot;, false}func ParseRequest(conn io.Reader) (*Request, error) { br := bufio.NewReader(conn) // ...省略部分代码... // 解析请求头部 for { line, err := br.ReadBytes('\\n') if err != nil { if err == io.EOF { break } } req.raw = append(req.raw, line...) line = bytes.TrimSpace(line) // \\r\\n if len(line) == 0 { break } colon := bytes.IndexByte(line, ':') // Bytes2Str 将 []byte 转换成 string req.Headers.Add(byteconv.Bytes2Str(bytes.TrimSpace(line[:colon])), byteconv.Bytes2Str(bytes.TrimSpace(line[colon+1:]))) } // ...省略部分代码... return req, nil} 获取到服务端的地址之后，建立 TCP 连接，然后将请求发送过去 123456789101112131415161718if !strings.Contains(host, &quot;:&quot;) { host += &quot;:80&quot;}server, err := net.Dial(&quot;tcp&quot;, host)if err != nil { conn.Close() log.Println(&quot;Dial server failed: &quot;, err) return}_, err = server.Write(request.Raw())if err != nil { log.Println(&quot;Write server failed: &quot;, err) conn.Close() server.Close() return} 最后将响应返回给客户端，直接调用 io.Copy 即可 123456tunnel(conn, server)func tunnel(client net.Conn, server net.Conn) { go io.Copy(server, client) go io.Copy(client, server)} 如果说，我们想对响应进行过滤或者其他的操作，我们应该进行解析，解析的过程和请求类似，如果需要过滤掉，则不将结果返回，返回一些错误码，比如 403 即可。 对于 HTTPS 协议来说其实也是差不多的，但是由于 HTTPS 采用 SSL/TLS 对数据进行加密，所以我们无法对其实际的请求进行解析，不过对 CONNECT 请求进行解析已经可以满足我们的需求了。 接受到 CONNECT 请求之后，我们首先应该返回 2xx 表示连接成功，然后将请求的数据原封不动的发送给服务端，服务端返回的也原封不动的返回给客户端，我们对其中的数据不关心，即使关心也解密不了。 12345if request.Method == &quot;CONNECT&quot;{ conn.Write([]byte(&quot;HTTP/1.1 200 OK\\r\\n\\r\\n&quot;)) tunnel(conn, server) return} 进行测试下载插件 Proxy SwitchyOmega，如果你使用 Edge，见 这里，如果你使用Google，见 这里 打开插件进行配置，如下 访问一个网址，比如http://httpbin.org，如果可以访问，那么说明代理服务器生效~ 完整代码见 GitHub","link":"/2021/12/06/network/http-proxy/"},{"title":"HTTP 协议简介","text":"什么是HTTP协议HTTP 是 Hypertext Transfer Protocol 的缩写，也就是超文本标记协议，多数的 HTTP 协议运行在 TCP 上，新型的 HTTP3 协议运行在 UDP 上。 简单来说，HTTP 协议就是一个 客户端与服务端 通信的协议，客户端发送数据给服务端，那么服务端要认识这些数据表示什么，服务端返回给客户端的数据，客户端也要认识这些数据 HTTP发展HTTP其实诞生也不过30多年，但是对人类的影响是巨大的，随着技术的不断发展进步，HTTP协议也在不断地完善和改进 HTTP/0.9最古老的 HTTP 协议并不是 HTTP/1.0，而是 HTTP/0.9，但是 HTTP/0.9 并没有成为标准，也没有进行大规模的使用。HTTP/0.9 的组成极其简单，对于客户端，仅仅支持 GET 方法，服务端也只能返回 HTML 文本内容 HTTP/0.9 协议不能满足实际需求，也仅仅在实验室中使用 HTTP/1.01996年， HTTP/1.0 公布，HTTP/1.0 在 HTTP/0.9 的基础上进行了多方面的扩展，完善了请求行信息，添加了请求头部，响应内容也支持头部信息，和目前的协议格式基本一致 并且开始支持三个请求方法: POST: 向服务端发送数据 GET: 用来请求对应的资源 HEAD: 用于获取响应头部信息，与 GET 十分类似，但是没有响应主体 HTTP/1.0 默认使用的是短连接，也就是说，每一次向服务端请求一次资源，便需要重新创建一个新的 socket，向服务端请求数据，然后关闭 socket。 HTTP/1.1在 HTTP/1.0 发布没几个月，1997年，HTTP/1.1 也正式公布了，并且成为第一个标准化的 HTTP 协议。 HTTP/1.1 中默认开启长连接，也就是指定头部的 Connection:keep-alive ，使用长连接的好处就是可以减少 socket 创建的消耗，因为创建一个 socket 需要进行系统调用，这个需要消耗比较多的资源。 此外HTTP/1.1中增加了一系列其他的请求方法，比如 PUT,DELETE,OPTIONS 等等，并且可以使用头部的 Cache-Control 控制资源的缓存。 HTTP/1.1 在互联网中广泛使用，现在仍然有很多网站在使用 HTTP/1.1，我们可以使用 wireshark 进行抓包分析 HTTP/2HTTP/1.1 得到广泛使用，但是随着使用时间的越来越长，也暴露出了不少了的问题，比如说发送的内容都是明文字符，容易被第三方截获造成消息的泄露，在 HTTP/2 中采用了 SSL/TLS 协议，对数据进行加密，提高数据传输过程中的安全性，不过数据的加密需要较多的CPU资源和时间。 HTTP/1.1 在 socket 利用率方面还是有所缺陷，HTTP/2 中提出了 多路复用 机制，能够同时发送多个 HTTP 请求，然后服务端返回多个响应，如下图： 由于HTTP头部信息内容比较大，并且在很多时候发送的HTTP请求头部都有一部分的重复，HTTP/2中采用头部压缩技术 HPACK，使用字典对头部信息进行保存，这样下一次发送的时候就可以只发送对应的键就好了，大大压缩的头部消息。 HTTP/2 还支持 服务端推送，服务端在客户端请求资源的时候可以推断下一次客户端可能需要的资源，发送给客户端缓存到本地，下一次使用的时候可以直接使用缓存中的资源，当然客户端也可以拒绝服务端推送的资源。 实现简单的HTTP客户端和服务端其实服务端和客户端的沟通就是通过 socket 进行连接，然后通过发送的数据和解析数据，客户端和服务端便可以互相理解，从而做出对应的动作，这里为了简单，使用 HTTP/1.1 协议作为讲解，请求网站 http://httpbin.org 客户端根据我们上面的理解，客户端请求数据的时候有一个请求行和一个请求头部，我们只需要满足这个条件，便可以模拟浏览器发送一个 HTTP 请求了。 首先我们先与服务端建立连接 1234567// 使用host:port形式conn, err := net.Dial(&quot;tcp&quot;, &quot;httpbin.org:80&quot;)if err != nil { fmt.Println(&quot;Dial tcp err: &quot;, err) return }defer conn.Close() 然后构建HTTP请求数据 123456msg := strings.Builder{}msg.WriteString(&quot;GET /get HTTP/1.1\\r\\n&quot;)msg.WriteString(&quot;Host: httpbin.org\\r\\n&quot;)msg.WriteString(&quot;Accept: application/json\\r\\n&quot;)msg.WriteString(&quot;Connection: close\\r\\n&quot;)msg.WriteString(&quot;\\r\\n&quot;) 接着将数据发送到服务端 12345_, err = conn.Write([]byte(msg.String()))if err != nil { fmt.Println(&quot;Send msg err: &quot;, err) return } 最后读取服务端返回的数据即可 123456789101112buf := bufio.NewReader(conn)for { msg, err := buf.ReadString('\\n') if err != nil { if err == io.EOF{ break } fmt.Println(&quot;Read err: &quot;, err) return } fmt.Print(&quot;Receive: &quot;, msg)} 服务端对于服务端来说，我们需要解析客户端请求的数据，然后进一步做出响应，这里为了简单，仅仅向客户端显示一个 Hello World 字符串。 首先我们需要监听一个端口 12345listener, err := net.Listen(&quot;tcp&quot;, &quot;:8888&quot;)if err != nil { fmt.Println(&quot;Listen err: &quot;, err) return} 然后接收客户端的请求，得到一个与客户端的连接 123456789for { conn, err := listener.Accept() if err != nil { fmt.Println(&quot;Accept err: &quot;, err) return } // 对于每一个连接，起一个goroutine进行处理 go handle(conn)} 然后向连接中写入HTTP响应格式的数据即可 123456789func handle(conn net.Conn){ defer conn.Close() msg := strings.Builder{} msg.WriteString(&quot;HTTP/1.1 200 OK\\r\\n&quot;) msg.WriteString(&quot;\\r\\n&quot;) msg.WriteString(&quot;Hello World\\r\\n&quot;) msg.WriteString(&quot;\\r\\n&quot;) conn.Write([]byte(msg.String()))} 使用curl命令请求便可以获取到对应的响应信息 12$ curl http://localhost:8888Hello World","link":"/2021/07/03/network/http/"},{"title":"Rust 参数设置默认值","text":"Option 模式？在调用函数的时候，可能参数比较多，但是我们只需要修改其中的几个特定参数就可以满足我们的需求，而其他的参数我们希望保持不变，但是 Rust 和 Go 一样，函数调用中不支持默认参数，所以我们必须另寻他法，那么第一感觉我们是不是可以采用 Go 中的 选项模式呢？ 虽然 Rust 函数中并不支持变参数传递，但是我们可以通过 宏 来间接达到相同的功能，代码如下 12345678910111213141516171819202122232425262728#[derive(Debug)]pub struct Server { host: String, port: i16,}impl Default for Server { fn default() -&gt; Self { Self { host: &quot;127.0.0.1&quot;.to_string(), port: 8888, } }}macro_rules! NewServer { ( $($f:ident), *) =&gt; {{ let mut s = Server { ..Default::default() }; $( $f(&amp;mut s); )* s }};} 我们只要传递函数给宏，函数中修改参数的指即可，但是我们如何将函数传递进去呢，这里我们采用 FnOnce 闭包函数 1234567fn with_host(host: String) -&gt; impl FnOnce(&amp;mut Server) { move |server| server.host = host}fn with_port(port: i16) -&gt; impl FnOnce(&amp;mut Server) { move |server| server.port = port} 调用的时候不太方便，因为宏中不支持使用括号，所以必须使用下面的方式进行调用 1234567#[test]fn test() { let f = with_host(String::from(&quot;0.0.0.0&quot;)); let f2 = with_port(8000); let s = NewServer![f, f2]; println!(&quot;{:?}&quot;, s);} 函数重载在 Rust 中实际上并不支持函数重载，但是通过宏，我们可以达到类似的效果，但同时由于宏的替换方式，所以我们无法通过判断变量的类型，然后动态返回结果 123456789101112131415161718192021222324252627282930313233343536373839#[derive(Debug)]pub struct Server { host: String, port: i16,}#[macro_export]macro_rules! NewServer { () =&gt; { Server { host: String::from(&quot;127.0.0.1&quot;), port: 8080, } }; ($var: expr) =&gt; { Server { host: String::from($var), port: 8080, } }; (_, $var: expr) =&gt; { Server { host: String::from(&quot;127.0.0.1&quot;), port: $var, } };}#[test]fn test() { let s = NewServer!(); println!(&quot;{:?}&quot;, s); let s = NewServer!(&quot;0.0.0.0&quot;); println!(&quot;{:?}&quot;, s); let s = NewServer!(_, 8888); println!(&quot;{:?}&quot;, s);} 手动实现 setter创建对象的时候返回一个结构体，然后提供 setter 方法进行配置。 123456789101112131415161718192021222324252627282930#[derive(Debug)]pub struct Server { host: String, port: i16,}impl Server { pub fn new() -&gt; Self { Self { host: String::from(&quot;127.0.0.1&quot;), port: 8888, } } pub fn host(mut self, host: &amp;str) -&gt; Self { self.host = host.to_string(); self } pub fn port(mut self, port: i16) -&gt; Self { self.port = port; self }}#[test]fn test_new_server() { let server = Server::new().host(&quot;0.0.0.0&quot;).port(8080); println!(&quot;{:?}&quot;, server); } 使用 crate自己手动写了一个 derive macro，可以自动生成 getter 和 setter，代码仓库见 construct，在 Cargo.toml 中进行引入： 12[dependencies]construct = {git = &quot;https://github.com/junhaideng/construct&quot;} 然后创建结构体的时候添加 derive(Constructor) 即可，详细使用方式见代码仓库说明。 1234567891011121314151617181920212223242526272829303132mod server { use construct::Constructor; #[derive(Debug, Constructor)] pub struct Server { #[cons(setter = false, rename_getter = get_host)] host: String, #[cons(getter = false, rename_setter = set_server_port)] port: u16, } impl Server { pub fn new() -&gt; Self { Self { host: String::from(&quot;127.0.0.1&quot;), port: 8080, } } }}#[test]fn test() { use crate::server::Server; let mut s = Server::new(); println!(&quot;host: {}&quot;, s.get_host()); // not implement because rename // println!(&quot;port: {}&quot;, s.port()); s.set_server_port(10); println!(&quot;{:?}&quot;, s);} Builder 模式与之类似的方式，其实我们也可以通过 Builder(创建者) 模式达到相同的功能。 123456789101112131415161718192021222324252627282930313233343536373839struct Builder { host: String, port: u16,}struct Server { host: String, port: u16,}impl Builder { pub fn new() -&gt; Self { Self::default() } pub fn host(mut self, host: &amp;str) -&gt; Self { self.host = host.to_string(); self } pub fn port(mut self, port: u16) -&gt; Self { self.port = port; self } pub fn build(self) -&gt; Server { Server { host: self.host, port: self.port, } }}impl Default for Builder { fn default() -&gt; Self { Self { host: &quot;127.0.0.1&quot;.to_string(), port: 8080, } }}","link":"/2022/07/27/rust/default/"},{"title":"Git 基本使用","text":"概念当我们看到 Git 的时候，涌现上来的第一个想法就是 Git 是什么，我们为什么要使用 Git ？ 首先解决一下第一个问题，Git 是什么？ Git 官网上有一段说明: Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. 翻译一下就是说：Git 是一个免费的开源分布式版本控制系统，旨在快速且高效地处理从小型到大型的所有项目。 这里简单说明一下版本控制系统是什么，打个比方，我们有一段完整的代码，不过现在我们需要在此基础上进行新功能的开发，开发完成之后上线了，不过很不幸，上线之后出现了很多 bug，那应该怎么处理，不用想，第一件事肯定是回滚，采用之前运行正确的代码，如果我们没有采用 Git ，那我们就需要保存一份之前的源代码了，如果我们采用 Git ，那么我们可以很方便的将代码切换到历史中的每一条记录，这就叫做版本控制。 介绍完了什么是 Git ，那么我们为什么要用 Git 呢？ 首先，互联网公司都在用 Git ，当你参加工作的时候，无法避免地会使用 Git ，比如暑期在字节实习的时候，公司内部自己搭建了一个 Git lab，代码的提交都是使用 Git 进行的，你如果不知道怎么使用 Git ，那么你写好的代码就只能让其他同学帮忙上传，效率极低，而且同学不可能一直帮你吧，所以说学习 Git 很必要。 其次，Git 的分布式版本控制系统真的很棒，在公司里面一个项目往往是一组同学一块开发，不可能单枪匹马，不同的同学负责不同的模块，这个时候 Git 的作用就更明显了，开发不同 feature 的同学统一从仓库上拉取代码，然后在本地另外创建一个分支进行新功能开发，开发完成，测试通过，代码进行 review 之后合入主干分支，极大的提高了编码效率。 安装访问 Git 官网 下载，如果是 Linux 系统，基本上都会预装 Git ，如果没有，对于 Ubuntu 来说可以使用下面的命令进行安装。 1sudo apt install git -all 安装完成之后，进入命令行输入下方命令进行检查，如果输出版本号，则说明安装成功。 12$ git --versiongit version 2.25.1 使用首先介绍一下 Git 中的基本术语：工作区, 暂存区，版本库。 工作区：就是在电脑里能看到的目录，或者直接认为是我们在编写项目的目录。 暂存区：一般存放在 .git 目录下的 index 文件中，所以我们把暂存区有时也叫作索引 (index)。 版本库：工作区有一个隐藏目录 .git，不算工作区，而是 Git 的版本库，提交到这里都会形成一个版本，想要要切换到任意版本都很简单。 下面我们就开始介绍一些常用的命令 配置123456# 配置全局用户，去掉 --global 参数表示配置当前目录用户git config --global user.name &quot;xxx&quot;git config --global user.email &quot;xxx&quot;# 查看配置git config --list 基本操作创建仓库有两种方式，下面分别讲解： 一种是在本地直接创建，使用 git init，创建之后会生成一个 .git 目录 在当前目录进行创建：git init 在指定目录进行创建：git init repo 另外一种方式是从远程仓库克隆下来，使用 git clone 克隆到目录，目录名为仓库名：git clone 克隆到指定目录：git clone &lt;dir&gt; 添加文件git add 命令可将该文件添加到暂存区，可以支持多个文件以及通配符 123456# 添加一个文件git add README.md# 添加 src 目录下的文件git add src/# 添加当前目录所有git add . 查看状态git status 查看仓库当前的状态，显示有变更的文件。 123456789101112131415161718192021$ echo &quot;Hello World&quot; &gt; README.md$ git statusOn branch masterNo commits yetUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) README.mdnothing added to commit but untracked files present (use &quot;git add&quot; to track)$ git add README.md$ git statusOn branch masterNo commits yetChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: README.md 查看差异git diff 可以比较同一文件在暂存区和工作区的差异。 12345678$ git diff README.mddiff --git a/branch/README.md b/branch/README.mdindex ce01362..3b18e51 100644--- a/branch/README.md+++ b/branch/README.md@@ -1 +1 @@-hello+hello world 提交本地仓库git commit 可以将代码添加到本地仓库，添加到这里的代码之后可以恢复。 回退版本get reset 可以将代码回退到指定的版本，默认使用 --mixed 。 git reset --mixed：回退到指定版本，工作区文件内容保持不变，但是会删除暂存区。 git reset --soft：回退到指定的版本，保留工作目录和暂存区中的内容，并把重置 HEAD 所带来的新的差异放进暂存区。 git reset --hard： 撤销工作区中所有未提交的修改内容，将暂存区与工作区都回到指定版本。 查看提交日志git log 可以查看已经提交的日志记录 123456789$ git logcommit eaf922fa1908cb6e9295570252ce3776f851bc16 (HEAD -&gt; master, dev)Author: junhaideng &lt;201648748@qq.com&gt;Date: Fri Dec 17 19:21:53 2021 +0800 docs: add README.md $ git log --oneline7110de5 (HEAD -&gt; master) docs: add README.md 分支操作创建分支git branch &lt;branch name&gt; 或者 git checkout -b &lt;branch name&gt; 都可以创建分支 1234$ git checkout -b branch1Switched to a new branch 'branch1'$ git branch branch2Switched to a new branch 'branch2' 切换分支git checkout &lt;branch name&gt; 或者 git switch &lt;branch name&gt;，使用 git checkout - 可以切换到上一次所在的分支哦 1234$ git checkout branch1Switched to branch 'branch1'$ git switch branch2Switched to branch 'branch2' 查看分支git branch 查看所有的分支 1234$ git branch branch1 branch2* master 删除分支git branch -d/-D &lt;branch name&gt; 12$ git branch -D branch2Deleted branch branch2 (was eaf922f). 重命名分支git branch -m/-M &lt;old&gt; &lt;new&gt; 1234$ git branch -m branch1 dev$ git branch dev* master 合并分支git merge 可以将当前分支和指定分支提交合并 12345678910$ git branch* feat/say_hello master$ git checkout -Switched to branch 'master'$ git merge feat/say_helloUpdating 7110de5..19dad58Fast-forward README.md | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) 标签操作创建标签Git 中标签有两种： 轻量标签： git tag &lt;tag name&gt; 不保存其他任何信息，相当于某个提交的信息 附注标签： git tag -a &lt;tag name&gt; 可以显示打标签者的信息，时间和附注消息，然后显示具体的提交信息 也可以指定某个提交打标签: git tag xx &lt;commit id&gt; 删除标签删除本地标签git tag -d &lt;tag name&gt; 删除标签 12$ git tag -d v1.0Deleted tag 'v1.0' (was 15f051b) 删除远程标签12git push origin :refs/tags/&lt;tag name&gt;git push origin --delete &lt;tag name&gt; 查看标签git tag 可以显示所有的标签，我们也可以使用 git tag -l/--list，并且可以指定特定模式匹配 显示标签内容git show &lt;tag name&gt; 可以展示标签对应的提交信息，对于附注标签还可以显示添加的额外信息 1234567891011121314151617181920212223$ git tag v1.0 -m &quot;tag demo&quot;$ git tagv1.0$ git show v1.0tag v1.0Tagger: junhaideng &lt;201648748@qq.com&gt;Date: Sat Dec 18 09:43:27 2021 +0800tag democommit 54f9e2cbea36cdd28718a7a0ff847e005442142e (HEAD -&gt; master, tag: v1.0)Author: junhaideng &lt;201648748@qq.com&gt;Date: Fri Dec 17 20:26:25 2021 +0800 add: tagdiff --git a/tag/tag b/tag/tagnew file mode 100644index 0000000..ce01362--- /dev/null+++ b/tag/tag@@ -0,0 +1 @@+hello 远程仓库关联仓库123# 一般将 name 取为 origin# url 可以是 ssh,http 等git remote add &lt;name&gt; &lt;url&gt; 拉取最新提交12345# 只拉取最新提交git fetch # 拉取之后合并当前分支# 相当于 `Git` fetch + `Git` mergegit pull 推送最新提交本地写完代码之后，提交到仓库中 1git push [name] [branch name] 忽略某些文件有时候，我们有些文件不想要提交到仓库中间，比如说编译的中间仓库，那么我们可以创建一个 .gitignore 文件，并且在其中进行配置 12345678910# 忽略 .vscode 目录，无论根目录下的还是子目录下的.vscode/# 忽略根目录下的 target 文件夹/target/# 忽略所有的 .exe 文件*.exe# 指定忽略某个文件/src/hello.go# 不过滤 src 目录!src/ 更加详细的见：gitignore 规范Git 的提交应该符合一定的规定，这样方便操作，比如 1type: &lt;description&gt; type 一般为下面的类型： feat: 新功能（feature） fix: 修补bug docs: 文档（documentation） style: 格式（不影响代码运行的变动） refactor: 重构（即不是新增功能，也不是修改bug的代码变动） test: 增加测试 chore: 构建过程或辅助工具的变动 add: 添加某个文件等 change: 对某个文件进行改变，但不改变原来的功能 beautify: 对界面进行美化 此外也可以参考： 约定式提交 思维导图 注：图片来自网络","link":"/2021/12/18/git/basic/"},{"title":"如何使用 GitHub Actions 制作定时任务","text":"介绍GitHub Actions 是官方提供的一套 DevOps 工具，可以完成软件开发周期的任务，比如可以实现自动化测试，部署等，简单来理解，我们认为它给我们提供了一个服务器，在上面可以执行一系列的指令(不过会有一些限制)，这就意味着我们可以用来白嫖，比如做定时任务，又不需要自己买服务器。 下面介绍一下如果使用 GitHub Actions 制作一个定时任务，每天定时发布天气情况，当然你学会了之后可以用于签到啊🤭。 前置准备很简单，只要创建一个 GitHub 仓库，不妨叫做 scheduler， 然后将仓库 git clone 到本地编辑器进行编写即可。 逻辑代码实现这里我们做一个天气预报，每天都可以定时发送天气预报到我们的邮件里面去。 首先第一步，去哪里获取到天气的信息？网上有很多天气查询的网站，部分网站提供了对应的 API 接口，但是需要密钥或者限制了调用次数，对于简单的使用来说，比较适合，这里采用爬虫技术从百度天气上获取数据，不用受限于密钥和调用次数。 网站资源找到了，如何拉取数据下来？一般来说，写爬虫的时候想要获取数据的时候，我们会使用两种方式进行查看： 右键查看网页源代码 分析网络请求，F12 即可 这里使用第一种方法就可以获取到了，如下为网页源代码，直接包含天气数据： 然后我们可以使用正则表达式，将对应的数据部分获取下来，并且解析为 JSON 数据，大概代码如下： 123456789101112131415161718192021222324252627282930var dataPattern = regexp.MustCompile(&quot;window.tplData = (.*?);&lt;/script&gt;&quot;)// 省略天气数据定义....func getWeather(city string) (*WeatherData, error) { // 发送网络请求，获取源代码 resp, err := http.Get(fmt.Sprintf(&quot;http://weathernew.pae.baidu.com/weathernew/pc?query=%s&amp;srcid=4982&quot;, city)) if err != nil { return nil, err } defer resp.Body.Close() // 读取网页源码 data, err := io.ReadAll(resp.Body) if err != nil { return nil, err } // 进行匹配，获取数据 match := dataPattern.FindSubmatch(data) if len(match) == 0 { return nil, errors.New(&quot;Do not find weather data&quot;) } // 反序列化数据 var res WeatherData err = json.Unmarshal(match[1], &amp;res) if err != nil { return nil, err } return &amp;res, err} 解析之后，我们按照自己的想法拼接信息，然后将数据发送过去即可，这里采用邮件发送，代码如下，当然你可以使用包装更好的一些 package，这里直接使用标准库。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152func sendMail(to, content string) error { // 这里采用的是 126 邮箱，不同的邮箱 host 设置不同 auth := smtp.PlainAuth(&quot;&quot;, emailUsername, emailPassword, &quot;smtp.126.com&quot;) msg := fmt.Sprintf(&quot;From: %s\\r\\nTo: %s\\r\\nSubject: %s\\r\\nContent-Type: %s; charset=UTF-8\\r\\n\\r\\n %s&quot;, emailUsername, to, &quot;每日天气&quot;, &quot;text/plain&quot;, content, ) err := smtp.SendMail(&quot;smtp.126.com:25&quot;, auth, emailUsername, []string{to}, []byte(msg)) if err != nil { fmt.Println(&quot;send email failed: &quot;, err) return err } return nil}// 简单的示例，可以进行加工const tpl = `{{.city}}天气情况：今天是{{.date}}，农历{{.lunar}}，天气{{.weather}}，伴有{{.wind_direction}}，预测今日{{.precipitation_type}}紫外线{{.uv}}, {{.uv_info}}，pm2.5指标为{{.pm25}}, 属于{{.pm25_level}}`func main() { // ... t, err := template.New(&quot;weather&quot;).Parse(tpl) if err != nil { fmt.Println(&quot;Parse template failed: &quot;, err) return } var buf = &amp;strings.Builder{} // 将数据添加到模板中 err = t.Execute(buf, map[string]string{ &quot;city&quot;: city, &quot;date&quot;: data.Base.Date, &quot;lunar&quot;: data.Base.Lunar, &quot;weather&quot;: data.Weather[&quot;weather&quot;], &quot;wind_direction&quot;: data.Weather[&quot;wind_direction&quot;], &quot;uv&quot;: data.Weather[&quot;uv&quot;], &quot;uv_info&quot;: data.Weather[&quot;uv_info&quot;], &quot;precipitation_type&quot;: data.Weather[&quot;precipitation_type&quot;], &quot;pm25&quot;: data.PSPm25.PSPm25, &quot;pm25_level&quot;: data.PSPm25.Level, }) sendMail(to, buf.String())} 定时任务设置定时任务是这次的重点，设置起来其实也挺简单的。 首先我们需要创建文件夹 .github/workflows，文件夹下面可以创建工作流程文件，只要是 .yml 文件都会作为一个单独的工作流程文件，每一个文件都会在事件触发的时候执行，比如在提交代码，提PR的时候等。 123456789# 比如仅在push的时候执行on: push# push 或者 pull_request 的时候执行on: [push, pull_request]# 同时也可以指定分支on: push: branches: - main 更加具体的语法可以参加官方文档介绍，这里不再赘述，如果我们想要定义一个定时任务，那么触发事件可以类比下面的方式 123on: schedule: - cron: '30 5,17 * * *' # 每天5:30, 17:30执行，时间为国际标准时间，非北京时间！！ 其中 cron 语法和 Linux 中的一致，如下 123456789┌───────────── 分钟 (0 - 59)│ ┌───────────── 小时 (0 - 23)│ │ ┌───────────── 日期 (1 - 31)│ │ │ ┌───────────── 月 (1 - 12 or JAN-DEC)│ │ │ │ ┌───────────── 星期 (0 - 6 or SUN-SAT)│ │ │ │ ││ │ │ │ ││ │ │ │ │* * * * * 如果你不清楚输出的表示什么时间，那么你可以在 crontab guru 中进行测试。而对于我们的天气预报来说，我们的工作流程文件可以定义为： 1234567891011121314151617181920212223name: weather-report # 工作流程名称，会在网页中进行显示on: push: # 为了调试方便，修改代码之后push上去即可以看到效果 paths: # 指定只有哪些文件修改了才会触发该工作流程 - weather-report/** - .github/workflows/weather-report.yml schedule: # 定时任务 - cron: &quot;0 0 * * *&quot; # 每天 0 点跑 =&gt; 东八区 8点jobs: weather-report: runs-on: &quot;ubuntu-latest&quot; # 在什么机器上跑 env: # 设置的一些 secret，在 settings 中可以设置 EMAIL_USERNAME: ${{ secrets.EMAIL_USERNAME }} EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }} TO: ${{ secrets.TO }} steps: # 指定的步骤 - uses: actions/checkout@v2 - uses: actions/setup-go@v2 # 使用 golang with: go-version: &quot;^1.17&quot; # 版本号 - run: go run weather-report/main.go -eu $EMAIL_USERNAME -ep $EMAIL_PASSWORD -to $TO # 指定的命令 所有代码编写完成之后，push 到仓库中，便可以查看到正在执行的行为。 总结核心步骤其实很简单： 创建仓库，clone 下来 进行逻辑代码编写 创建 .github/workflows 目录，并编写工作流程文件 push 代码上去，事件触发时执行 工作流程文件的编写并不是很复杂，如果我们会 docker-compose.yml 的编写，很容易就掌握了基本内容，其他需要的语法到时候翻翻官方文档即可，而且官方提供了大量的模板供使用。 上述的这个定时任务并没有什么很大的使用之处，毕竟人手一台手机，天气点一点就是知道的事了，比这个方便美观多了，其实更重要是学习怎么利用爬虫去获取简单的数据，以及 GitHub Actions 的使用，如果把签到作为定时任务，那么定会得到极大的解放🧐。 不过，GitHub Actions 的使用当然不仅限于此，自动化测试，自动化部署都可以通过它实现~","link":"/2021/11/22/git/scheduler/"},{"title":"Vim 常见命令及其对应的含义","text":"command desc w 移动到下一个 word[^1] 的开头 e 移动到下一个 word 的结尾 W 移动到下一个 WORD 的开头 E 移动到下一个 WORD 的结尾 b 移动到上一个 word 的开头 B 移动到上一个 WORD 的开头 ge 移动到上一个 word 的结尾 gE 的移动到上一个 WORD 的结尾 f{char}[^2] 移动到当前行下一个 char 出现的位置 F{char} 移动到当前行上一个 char 出现的位置 t{char} 移动到当前行下一个 char 出现的前一个位置 T{char} 移动到当前行上一个 char 出现的前一个位置 0 移动到当前行的第一个字符位置 ^ 移动到当前行的第一个非空字符的位置 $ 移动到当前行的最后一个位置 g_ 移动到当前行的最后一个非空字符的位置 { 移动到下一个段落 } 移动到上一个段落 /{pattern} 搜索下一个符合的单词[^3] ?{pattern} 搜索上一个符合的单词 gg 移动到文件的最上方 {line}gg 移动到指定的行数 G 移动到文件的末尾 % 移动到匹配 ({[]}) 的位置[^4] dd 删除整行 D 删除到结尾的字符，相当于 d$ 使用 {count}{command} 效果相当于同时执行 count 个 command 命令，比如 5j 相当于往下移动 5 行. [^1]: 比如 words.，使用 word 表示的话，可以分成两个，word + .，但是如果使用 WORD 表示的话，则是一个。[^2]: 执行完命令之后，可以使用 ; 到下一个 char 出现的位置，使用 , 到上一个位置。[^3]: 使用回车进行确认，n 移动到下一个匹配的位置，N 移动到上一个匹配的位置。[^4]: 如果是 ({[ 则移动到 )}]，否则反之。","link":"/2023/09/03/tools/vim/"},{"title":"哈希函数","text":"概念什么是哈希函数？ 定义其实很简单，哈希函数其实就是一种映射关系，它可以把任意长度的数据映射为固定长度的数据，输出一般称为摘要(digest) ，就比如常见的 md5 算法，可以将输入映射成 128 bits。 $$\\{0, 1\\}^* \\overset{map}{\\longrightarrow} \\{0,1\\}^n$$ 同时，也正因为说，哈希函数将任意长度的输入映射成固定长度的输出，因为输入空间远大于输出空间，所以必然会存在冲突，比如下面的这种情景。不过，在实际使用的时候，必须要求哈希函数具备一定的安全性。 要求 以下只是几点比较重要的 计算 digest 比较快因为我们往往需要对大量的数据计算哈希值，如果计算速度慢，会影响整个系统的性能。 抗第一原像又称 Preimage resistance。 已知某个哈希值 $y$ 和哈希函数 $H$，找出 $x$ 满足 $H(x) = y$ 在计算上不可行。 抗第二原像性又称 Second-preimage resistance。 已知某个输入 $x$ 和哈希函数 $H$，找到 $x^{\\prime}$ 满足 $H(x^{\\prime}) = H(x)$ 在计算上不可行。 抗碰撞性又称 Collision resistance。 已知哈希函数 $H$，找出任意一对 $x^{\\prime}, x$， 且 $x^{\\prime} \\neq x$ 满足 $H(x^{\\prime}) = H(x)$，在计算上不可行 应用文件校验网站上的文件，在下载的时候，往往会提供一个摘要来检验文件是否下载完整，比如说下图^1的 SHA256SUMS 文件 数字签名数字签名有类似书写签名的作用，可以对数据提供身份校验以及完整性校验。 我们经常访问的 HTTPS 网站均提供了数字证书，为的就是校验这个网站颁发给我们的公钥是不是这个网站的，还是其他人伪造的一个公钥，点击网址签名的小锁 🔒。 进入 Connection is secure，然后点击右上角的证书图标，我们可以看到数字证书中的内容。 Go 中哈希函数使用示范Golang 中提供了许多哈希函数，比如 md5，sha1，sha256，sha512 等，并且通过接口 hash.Hash 暴露，使用起来十分简单，下面以 md5 做示范。 1234567891011121314151617package mainimport ( &quot;crypto/md5&quot; &quot;fmt&quot;)func main() { msg := []byte(&quot;Hello World&quot;) h := md5.New() h.Write(msg) fmt.Printf(&quot;Sum: %x\\n&quot;, h.Sum(nil)) // Sum: b10a8db164e0754105b7a99be72e3fe5}","link":"/2021/12/24/cryptography/signature/hash/"},{"title":"Lamport One Time Signature","text":"Lamport 是第一个 OTS(One Time Signature) 算法，由 Leslie Lamport 于 1979 年提出，论文可见 Constructing Digital Signatures from One Way Function ，一对密钥只能签名一次，不能重复使用。 下面按照 $(GEN, SIG, VER)$ 三元组描述该算法。 密钥生成($GEN$)$$(pk, sk) \\leftarrow GEN(1^k)$$ 首先我们需要确定安全参数 $k$，不过实际操作中我们一般只需要确定输出的摘要长度 $n$。 通过随机数生成器(PRG, Pseudo Random Generator)，生成 $n$ 对随机数，每一对包含两个随机数，每一个随机数 $n$ bits，总共 $2\\times n\\times n$ bits，此为私钥。 对每一个随机数进行哈希，生成 $2\\times n$ 个摘要，每一个 $n$ bits，也是 $2 \\times n \\times n$ bits，此为公钥。 形式化表示为： $$sk = (sk_{0,0}, sk_{0,1}, \\dotsm, sk_{n-1, 0}, sk_{n-1, 1}) \\\\pk = (pk_{0,0}, pk_{0,1}, \\dotsm, pk_{n-1, 0}, pk_{n-1, 1})$$ 图示： 消息签名($SIG$)$$ \\sigma(M) \\leftarrow SIG(M, sk)$$ 对于输入 $M \\in \\{0, 1\\}^*$，首先使用哈希函数进行处理，生成 $n$ bits 的摘要。 $$m = (m_0, m_1, \\dotsm, m_{n-1})$$ 对于摘要中的每一个 bit，进行下面的操作：对于摘要中的第 $i$ 个 bit $m_i$，如果该 bit 为 0，那么取 $sk_{i,0}$，否则该 bit 位为 1，取 $sk_{i,1}$，最后签名为 $$\\sigma =(\\sigma_0, \\sigma_1,\\dotsm \\sigma_{n-1}) = (sk_{0,m_0}, sk_{1,m_1}, \\dotsm, sk_{n-1,m_{n-1}})$$ 比如说输入的 $M$ 经过哈希之后得到 $$(0,1,0,1,1,0,0,1)$$ 那么签名结果是 $$\\sigma= (sk_{0,0}, sk_{1,1}, sk_{2,0},sk_{3,1},sk_{4,1},sk_{5,0},sk_{6,0},sk_{7,1})$$ 图示： 消息校验($VER$)$$ false/true \\leftarrow VER(M, \\sigma(M), pk) $$ 对于消息 $M \\in \\{0,1\\}^*$，首先进行哈希，得到 $n$ bits 的摘要。 $$m = (m_0, m_1, \\dotsm, m_{n-1})$$ 对于摘要中的每一个 bit，进行下面的操作：对于摘要中的第 $i$ 个 bit $m_i$，如果该 bit 为 0，那么取 $pk_{i,0}$，否则该 bit 位为 1，取 $pk_{i,1}$，最后得到 $$v= (pk_{0,m_0}, pk_{1,m_1}, \\dotsm, pk_{n-1,m_{n-1}})$$ 对于签名 $\\sigma$ 中的每一个部分进行哈希，得到 $$v^{\\prime}= (pk_{0,m_0}^{\\prime}, pk_{1,m_1}^{\\prime}, \\dotsm, pk_{n-1,m_{n-1}}^{\\prime})$$ 如果 $v=v^{\\prime}$，则签名校验通过，返回 $true$，否则校验失败，返回 $false$。 图示：","link":"/2021/12/24/cryptography/signature/lamport/"},{"title":"Merkle Tree &amp; Merkle Signature Scheme","text":"概念Merkle Tree 又称 Hash Tree，实现一般为二叉树，当然也可以用多叉树实现，本质是一样的，WiKi 上也有相关介绍，点击这里传送。 树的叶子节点处存放数据的哈希值，其他的非叶子节点通过子节点进行构造，按照下方公式： $$node_i = hash(node_{2i+1} || node_{2i+2})$$ 其中，节点 $i$ 是节点 $2i+1$ 和 $2i+2$ 的父结点，$||$ 表示串接，或者简单的说拼接，比如 $a = 0001_2, b = 1100_2$ 则$$ c = a || b = 00011100_2$$ Merkle Tree 的结构如下： 作用Merkle Tree 主要有以下两个作用。 1. 校验数据的完整性 现在比如说有一个文件很大，并没有存放在单独的一台机器上，而是分布在各个网络主机上，如果我们想要获取到该完整的文件，必须从各个主机上获取，然后进行合并，但是我们如何保证获取的文件是完整的，没有丢失任何数据？ 我们可以提供数据的摘要来确保。每一份文件块均含有一个摘要，我们获取到该文件块的时候重新计算摘要，并且与之进行匹配，如果摘要不相等，说明文件块不对，需要重新传输。 PS: 如果不需要防止主动攻击，比如消息篡改，使用 CRCs 等校验和算法即可。 但是这样，我们需要从可信方获取获取所有的摘要值，代价有点高。 更好的方式是采用 Merkle Tree，我们首先会从可信方获取到文件的根哈希值 $root$，在每一个主机发送的内容中会提供当前数据的摘要，以及在 Merkle Tree 中的 Authentication path (下文会提到)，下载完数据之后，会根据 Authentication path 计算根哈希值 $root^\\prime$，如果和 $root$ 不匹配，说明该文件下载错误，进行错误处理，重试或者放弃下载，如果匹配，则会继续下载。 2. 减少签名中公钥的大小 在没有使用 Merkle Tree 之前，比如我们需要保存 $2^h \\times n$ bits 的公钥，可以认为是树中的所有叶子节点，但是使用 Merkle Tree，公钥变成了树的根节点，也就是只需要保存 $n$ bits，在 $h$ 达到一定大小的时候，节约的空间大小十分可观。 Authentication pathMerkle Tree 中存在一个比较重要的概念：Authentication path，指的是，叶子节点到根节点经过的路径上的节点的兄弟节点集合。举个栗子，下面节点 $10$ 到根节点经过的节点有 $[10, 4, 1, 0]$，分别对应的兄弟节点为 $[9, 3, 2]$，即为节点 $10$ 的认证路径。 通过已知节点和 Authentication path，我们可以计算出根节点的值，比如上面的这个例子中，执行下面的步骤，即可求出根节点： 由 $10$ 和 $9$ 计算出 $4$ 的值 由 $4$ 和 $3$ 计算出 $1$ 的值 由 $1$ 和 $2$ 计算出根节点 $0$ 的值 Merkle Signature SchemeMerkle Signature Scheme 由 Ralph Merkle 于 1979 年提出，简称 MSS，并不是一种单独的一种签名算法，而是整合了 Merkle Tree 和 OTS 形成的，论文见 A Certified Digital Signature。 下面按照三元组 $(GEN, SIG, VER)$ 对 MSS 签名系统进行介绍。 密钥生成($GEN$)$$(pk, sk) \\leftarrow GEN(1^k)$$ 首先，我们需要确定树的高度 $h$，然后生成 $2^h$ 个 OTS 密钥对 $(X_j,Y_j)$，$j \\in [0, 2^h-1]$。 依次计算值 $v_j = hash(Y_j)$，作为 Merkle Tree 的叶子节点，并且不断利用下方公式直至计算出根节点 (root) 的值，即为公钥 pk。 $$node_i = hash(node_{2i+1} || node_{2i+2})$$ 私钥为 $2^h$ 个 OTS 私钥 $X_j$。 图示： 消息签名($SIG$)$$ \\sigma(M) \\leftarrow SIG(M, sk)$$ 对于输入的消息 $M \\in \\{0,1\\}^*$，首先挑选出一个 OTS 密钥对，索引为 $i, i \\in [0, 2^h-1]$ ，该密钥对之前未被使用，一般按序选择，从左到右，然后使用该 OTS 私钥 $X_i$ 对消息进行签名，得到 $\\sigma_{OTS}$，并且求出对应的公钥 $Y_i$ 的 Authentication path，记为 $auth_i$。 则签名为 $$\\sigma = (i, \\sigma_{OTS}, Y_i, auth_i)$$ 图示： 消息校验($VER$)$$ false/true \\leftarrow VER(M, \\sigma(M), pk) $$ 第一步，签名中包含 OTS 的公钥 $Y_i$，使用$$false/true \\leftarrow VER(M, \\sigma_{OTS}, Y_i) $$ 检验 $\\sigma_{OTS}$，如果不通过，返回 $false$，停止计算，否则进行下一步。 第二步，根据 $i$ 和 $Y_i$，计算出 Merkle Tree 的根节点 $root^\\prime$，与 $pk$ 进行比较，如果相等，校验通过，返回 $true$，否则返回 $false$。 图示：","link":"/2021/12/26/cryptography/signature/merkle/"},{"title":"HORS with trees (HORST)","text":"HORST 属于 FTS，由 HORS 改进而来，相比较 HORS 而言，牺牲了运行时间，但是减少了公钥的大小，同时算法中引入了掩码的计算。 HORST 中的公钥是二叉哈希树 (binary hash tree) 的根节点，叶子节点是 HOSR 公钥的 $(t=2^\\tau)$ 个 $block$ 。 本文的介绍建立在 HORS 之上，若还尚未了解 HORS 签名系统机制，可以参考另一篇文章 Hash to Obtain Random Subset (HORS)，此外，提出 HORST 的论文见 SPHINCS: practical stateless hash-based signatures。 下面按照三元组 $(GEN, SIG, VER)$ 介绍该签名算法。 密钥生成 (GEN)$$pk \\leftarrow GEN(\\mathcal{S}, \\mathcal{Q})$$ 首先，我们需要确定参数 $t = 2^\\tau$ 和 $k$，且 $t \\times k = m$，$m$ 为消息哈希之后的长度，使用哈希函数 $\\mathcal{H}$，$n$ 为树中的哈希值长度，使用哈希函数 $\\mathcal{F}$。然后求出使得 $k(\\tau -x +1) + 2^x$ 最小的 $x$，如果 $x$ 有两个解，取较大的那个。 在 $SPHINCS-256$ 中 $$t = 2^{16}, k = 32, m = 512, n = 256, x = 6 $$ 下面设 $$\\mathcal{H}: \\{0,1\\}^* \\rightarrow \\{0,1\\}^m$$ $$\\mathcal{F}: \\{0,1\\}^* \\rightarrow \\{0,1\\}^n$$ 密钥对的生成需要两个输入，一个是随机数种子 $\\mathcal{S}\\in \\{0,1\\}^n$，另一个是掩码 $\\mathcal{Q} \\in \\{0,1\\}^{2n\\times \\log t}$。 其中 $\\mathcal{Q}$ 可以表示为 $$ \\mathcal{Q} = (\\mathcal{Q}_{0,0}, \\mathcal{Q}_{0,1}, \\dotsm, \\mathcal{Q}_{\\log t-1,0}, \\mathcal{Q}_{\\log t-1,1}) $$ 首先，随机生成 $t$ 个随机数，作为私钥： $$sk = (sk_0, sk_1, \\dotsm, sk_{t-1}) \\leftarrow G_t(\\mathcal{S}) $$ 然后计算 $$L_i = \\mathcal{F}(sk_i)$$ 作为哈希树的叶子节点，计算出树的根节点，作为公钥，这样的计算公式和 Merkle Tree 不太一样，因为需要计算掩码。 $\\mathcal{Q}$ 一共是 $2 \\log t$ 个，每一个 $n$ bits，每一层对应一对掩码 $(\\mathcal{Q}_{i,0}, \\mathcal{Q}_{i,1})$。 $$node_i = \\mathcal{F}(node_{2i}\\oplus \\mathcal{Q}_{i,0}|| node_{2i+1}\\oplus \\mathcal{Q}_{i,1})$$ 左节点使用 $\\mathcal{Q}_{i,0}$ 进行异或操作，右结点则使用 $\\mathcal{Q}_{i,1}$。 图示： 消息签名 (SIG)$$(\\sigma, pk) \\leftarrow (M, \\mathcal{S}, \\mathcal{Q})$$ 对于输入的任意消息，首先使用哈希函数 $\\mathcal{H}$ 转换成 $M \\in \\{0,1\\}^m$，然后将 $M$ 分成 $k$ 份，每一份 $\\log t$ bits。 $$M = (M_0, M_1, \\dotsm, M_{k-1})$$ 每一份 $M_i, i \\in [0, k-1]$ 对应一个整数。 与 HORS 不同的是，HORST 签名 $\\sigma$ 包含 $k+1$ 个 $blocks$： $$\\sigma = (\\sigma_0, \\sigma_1, \\dotsm, \\sigma_k)$$ 对于 $i \\in [0,k-1], \\sigma_i = (sk_{M_i}, Auth_{M_i})$，其中 $sk_{M_i}$ 的选择方法和 HORS 中一致， $Auth_{M_i}$ 是叶子节点到 $\\boldsymbol{\\tau -x}$ 层的 认证路径，而不是根节点。 对于 $i=k$，$\\sigma_k$ 为 $\\tau -x$ 层的$2^x$ 个所有节点值： $$\\sigma_k = (N_{0,\\tau-x},N_{1,\\tau-x},,\\dotsm, N_{2^x-1,\\tau-x})$$ 图示： 消息校验 (VER)$$pk^\\prime \\leftarrow (M, \\sigma, \\mathcal{Q})$$ 根据上述处理，得到$$M = (M_0, M_1, \\dotsm, M_{k-1})$$ 对于 $i \\in [0, k-1], y_i = \\lfloor \\frac{M_i}{2^\\tau}-x\\rfloor$，通过 $L_{M_i} = \\mathcal{F}(\\sigma^1_i)$ 和 $Auth_{M_i} = \\sigma^2_i$ 计算出 $N^\\prime_{y_i, \\tau-x}$，和 $\\sigma_k$ 中对应的进行比较，如果全部匹配，则返回根节点 $pk$，否则返回 $fail$。 其中 $\\sigma_i^1$ 表示签名第 $i$ 个 $block$ 的第一部分内容，也就是私钥内容，$\\sigma_i^1$ 表示签名第 $i$ 个 $block$ 的认证路径。 图示：","link":"/2021/12/27/cryptography/signature/horst/"},{"title":"Hash to Obtain Random Subset (HORS)","text":"HORS(Hash to Obtain Random Subset)属于 FTS, 其生成的公私钥对，可以多次用来签名消息，但是每一次签名之后，系统的安全性会降低，签名能够被伪造的概率上升。论文可见 Better than BiBa: Short One-time Signatures with Fast Signing and Verifying。 下面以三元组 $(GEN, SIG, VER)$ 对 HORS 签名系统进行介绍。 密钥生成($GEN$)$$(pk, sk) \\leftarrow GEN(1^k)$$ 首先，我们需要确定参数 $k$ 和 $t = 2^\\tau$，哈希函数输出的摘要长度为 $n = k \\times \\log_2t = k \\times \\tau$ bits。 这里，我们一般要求 $8\\mid \\tau$，也就是 $\\tau$ 应该是 $8$ 的倍数。 通过 $PRG$ 生成 $t$ 个随机数，构成私钥： $$sk = (sk_0, sk_1, \\dotsm, sk_{t-1})$$ 对每一个随机数进行哈希，得到公钥 $$\\begin{aligned} pk &amp;= (pk_0, pk_1, \\dotsm, pk_{t-1}) \\\\ &amp;= (hash(sk_0), hash(sk_1), \\dotsm, hash(sk_{t-1}))\\end{aligned}$$ 图示： 消息签名($SIG$)$$ \\sigma(M) \\leftarrow SIG(M, sk)$$ 对消息 $M\\in \\{0, 1\\}^*$ 进行哈希，得到摘要 $m$。 将 $m$ 进行分割，分成 $k$ 份，每一份 $\\tau = \\log_2t$ bits，这里如果 $8|\\tau$，那么编写程序的时候会得到极大的简化。 $$m = (m_0, m_1, \\dotsm, m_{k-1})$$ 将每一份转换成对应的整数，比如每份 $16$ bits 时： $$\\begin{aligned} 0001\\_0010\\_1111\\_1000_2 = 4856_{10}\\end{aligned}$$ 则签名为 $$\\begin{aligned} \\sigma &amp;= (\\sigma_0, \\sigma_1, \\dotsm, \\sigma_{k-1}) \\\\ &amp;= (sk_{m_0}, sk_{m_1}, \\dotsm, sk_{m_{k-1}})\\end{aligned} $$ 其中有可能$$ \\exists \\ i, j \\in [0, k-1], i \\ne j, \\sigma_i = \\sigma_j $$ 图示： 消息校验($VER$)$$ false/true \\leftarrow VER(M, \\sigma(M), pk) $$ 同样，将 $M$ 进行哈希之后，转换成 $k$ 个 $\\tau$ bits 的整数 $$m = (m_0, m_1, \\dotsm, m_{k-1})$$ 计算出 $$ pk^\\prime = (pk^\\prime_{m_0}, pk^\\prime_{m_1}, \\dotsm, pk^\\prime_{m_{k-1}}) $$ 对于 $i \\in [0,k-1]$，如果 $hash(\\sigma_i) = pk^\\prime_i$ 均成立，则校验成功，返回 $true$，否则校验失败，返回 $false$。 图示：","link":"/2021/12/25/cryptography/signature/hors/"},{"title":"签名算法","text":"定义任意的签名算法都可以用一个三元组表示: ($GEN, SIG, VER$)，可以说是组成签名系统的三个算法。 $GEN$: 密钥生成算法 $SIG$: 消息签名算法 $VER$: 消息校验算法 定义一个接口表示： 12345678type Signature interface { // GenerateKey generates secret key and public key GenerateKey() (sk []byte, pk []byte) // Sign computes signature of message using secret key Sign(message []byte, sk []byte) []byte // Verify check if the signature is valid Verify(message []byte, pk []byte, signature []byte) bool} 下面详细说说这三个算法。 密钥生成算法形式化表示为： $$(pk, sk) \\leftarrow GEN(1^k)$$ 其中，$k$ 是安全参数，wiki 上给出了详细的说明： In cryptography, the security parameter is a variable that measures the input size of the computational problem. Both the resource requirements of the cryptographic algorithm or protocol as well as the adversary’s probability of breaking security are expressed in terms of the security parameter. 也就是相当于说，安全性参数可以表示出破坏该签名安全性的概率和需要的资源等。 不过在实际中，我们可能会固定 $k$，比如为 256 等等。 指定安全参数之后，调用该方法，系统可以生成一对密钥，其中包括一个私钥和公钥，公钥可以公布，但是私钥必须安全保存。 消息签名算法形式化表示为：$$ \\sigma(M) \\leftarrow SIG(M, sk)$$ 输入需要签名的消息，以及生成的私钥，该算法可以输出消息的签名。 消息校验算法形式化表示为：$$ false/true \\leftarrow VER(M, \\sigma(M), pk) $$ 输出消息，和对应需要校验的签名，然后输入公钥，该算法可以确定消息是否完整，并可以校验发布者的身份，因为只用该发布者私钥签名的数据，才能成功通过该公钥解密。 满足一个签名算法必须满足两个性质： 不可伪造性：其他用户不可能伪造签发者对数据进行签名。 正确性：通过上述三个步骤进行计算，输出结果一定为 $true$。 Hash-based 签名方案分类One Time Signature (OTS)$OTS$ 的每一个密钥对，只能用来签名一条消息，因为在签名完第一次消息之后，该密钥对不再安全，攻击方可以 100% 伪造签名。 比如：Lamport OTS, WOTS. Few Times Signature (FTS)$FTS$ 的每一个密钥对，可以用来签名多个消息，但是随着签名的次数增加，安全性会急剧下降。 比如：HORS.","link":"/2021/12/24/cryptography/signature/signature/"},{"title":"SPHINCS","text":"SPHINCS 于 2015 年由 Bernstein, Daniel J., Daira Hopwood, Andreas Hülsing 等人提出，具体的论文见 SPHINCS: practical stateless hash-based signatures。 SPHINCS 是一个无状态的签名系统。有状态指的是每次签名时需要记录系统的信息，比如是第几次签名，在验证的时候需要使用该参数，而无状态的签名系统则不需要保存这些信息。相对于有状态的签名系统，无状态的签名系统不需要记录签名的状态，符合标准的 API 且方便在不同主机上移植，无状态的签名系统是更好的选择。 SPNHICS 基于多种签名算法，比如 WOTS+, HORST 等，如果不了解相关内容，建议先参考之前的文章。 SPHINCS 的结构大致如下： 每一个层的 Tree 都是一个 binary hash tree，叶子节点是 WOTS+ 公钥通过 L-Tree 形成的根节点，这里的密钥对用来认证，最下面的 HORST 用来签名消息。 SPHINCS 中使用 address scheme 来进行伪随机密钥的生成，一个地址(address) 由三个部分组成： 其中，Tree 部分的层数，从上到下，依次是 $d-1,d-2, \\dotsm, 0$，HORST 属于 $d$ 层。 下面通过 $(GEN, SIG, VER)$ 三部分进行讲解。 密钥生成 (GEN)$$(SK, PK) \\leftarrow GEN(1^k)$$ 首先，在 SPHINCS 中存在下面的几个参数: paramter meaning $n$ biglength of hashes in HORST and WOTS $m$ bitlength of the message hash $h$ height of the hyper-tree $d$ layers of the hyper-tree $w$ Winternitz parameter used for WOTS signatures $t$ number of secret-key elements of HORST $k$ number of revealed secret-key elements per HORST sig. 在 $SPHINCS-256$ 中，分别为 $$n=256, m = 512, h=60,d=12,w=16,t=2^{16},k=32$$ 由 PRG 生成两个随机数作为私钥： $$(SK_1, SK_2) \\in \\{0, 1\\} ^ n \\times \\{0,1\\}^n$$ $SK_1$ 用来生成密钥，$SK_2$ 在签名的时候用来生成不可预测的 $index$ 和随机化消息的哈希值。 然后生成 $p$ 个随机数作为掩码 $$Q = (Q_0, Q_1, \\dotsm, Q_{p-1}) \\leftarrow \\{0, 1\\} ^{p\\times n}$$ 且 $p$ 满足 $$ p = \\max\\{2^w-1, 2(h+\\lceil \\log l \\rceil), 2 \\log t\\}$$ 并且设 $$\\begin{aligned} Q_{WOTS+} &amp;= (Q_0, Q_1, \\dotsm, Q_{2^w-2}) \\\\ Q_{HORST} &amp;= (Q_0, Q_1, \\dotsm, Q_{2\\log t-1}) \\\\ Q_{L-Tree} &amp;= (Q_0, Q_1, \\dotsm, Q_{2\\lceil \\log l \\rceil-1}) \\\\ Q_{Tree} &amp;= (Q_{2\\lceil \\log l \\rceil},Q_{2\\lceil \\log l \\rceil+1}, \\dotsm, Q_{2(h+\\lceil \\log l \\rceil)-1} )\\end{aligned} $$ 即 $Q_{WOTS+}$ 为 $Q$ 的前 $2^w-1$ 个， $Q_{HORST}$ 为 $Q$ 的前 $2 \\log t$ 个，$Q_{L-Tree}$ 为前 $2\\lceil \\log l \\rceil$ 个，$Q_{Tree}$ 为 $Q_{L-Tree}$ 随后的 $2h$ 个。 最后，我们需要生成最上层的 Tree，也就是 layer d-1 的树。叶子节点为 WOTS+ 密钥对，通过上述的 address scheme 计算叶子节点的地址： $$A = (d-1||0||i), i \\in [0, 2^{h/d}-1]$$ 随后可以计算出对应的随机数种子： $$S_A \\leftarrow \\mathcal{F_a}(A, SK_1)$$ 此处，将 $\\mathcal{F_a}$ 认为是一个哈希函数即可。 然后我们就可以生成对应的公钥了 $$pk_A \\leftarrow WOTS.kg(S_A, Q_{WOTS+})$$ 生成的 $$pk_A$$ 由 $l$ 个 $blocks$ 构成，将其构造成一个 L-Tree，根节点就是在 Tree 上的叶子节点值。 就这样，我们可以获取到 $Tree_0$ 的根节点值 $PK_1$。 从而，SPHINCS 的公私钥为： $$\\begin{cases} SK = (SK_1, SK_2, Q) \\\\ PK = (PK_1, Q)\\end{cases}$$ 图示： 消息签名 (SIG)$$\\Sigma \\leftarrow sign(M,SK)$$ 对于输入的 $M\\in \\{0, 1\\}^*, SK = (SK_1, SK_2,Q )$ ，首先计算消息的随机摘要，利用 $M,SK_2$ 生成一个随机数 $$R = (R_1, R_2) \\in \\{0, 1\\}^n \\times \\{0,1\\}^n \\leftarrow \\mathcal{F}(M, SK_2)$$ 取 $R_1$ 计算消息的随机摘要： $$D \\leftarrow \\mathcal{H}(R_1, M)$$ 取 $R_2$ 的前 $h$ bits 选择 HORST 密钥对 $$i \\leftarrow C_{HOP}(R_2,h)$$ 因为树高 $h$，$h$ bits 包含 $2^h$ 个数，正是 HORST 的数量。 已知 $i$ 之后，我们可以计算该 HORST 的地址 $$A_{HORST} = (d || i(0, (d-1)h/d)||i((d-1)h/d,h/d))$$ 其中， $i(start, length)$ 表示从 $start$ 比特位开始，截取 $length$ 长度的比特值。 同理，计算出对应的随机数种子： $$S_{HORST} \\leftarrow \\mathcal{F_a}(A_{HORST}, SK_1)$$ 根据随机数种子和掩码 $Q$，我们可以获取到： $$(\\sigma_H, pk_H) \\leftarrow HORST.sign(D, S_{A_{HORST}}, Q_{HORST})$$ 对于 layer $j, j \\in [0, d)$，计算 WOTS+ 密钥对的地址： $$A_j = (j || i(0, (d-1-j)h/d) || i((d-1-j)h/d,h/d))$$ 计算出地址之后，求出对应的随机数种子 $$S_{A_j} = \\mathcal{F_a}(A_j, SK_1)$$ 然后计算签名： $$\\sigma_{w,j} \\leftarrow WOTS.sign(pk_{w,j-1}, S_{A_j}, Q_{WOTS+})$$ $pk_{w,j-1}$ 就是 Tree 的根节点，这里取 $\\sigma_{-1} = pk_H$，也就是 HORST 的根节点，同时，我们还需要求出 WOTS+ 密钥对的认证路径 $Auth_{A_{d-1}}$。 最后得出，签名为： $$\\Sigma = (i, R_1, \\sigma_H, \\sigma_{W, 0}, Auth_{A_0}, \\dotsm, \\sigma_{W,d-1}, Auth_{A_{d-1}})$$ 图示： 消息校验 (VER)$$b\\leftarrow VEF(M,\\Sigma, PK)$$ 对于消息 $M\\in \\{0,1\\}^*$ 计算出摘要 $D \\leftarrow \\mathcal{H}(R_1, M)$。 然后首先校验 $\\sigma_H$： $$pk_H \\leftarrow HORST.verify(D, \\sigma_H, Q_{HORST})$$ 如果校验没通过，返回失败，否则进行下一步。 计算 WOTS+ 的公钥 $$pk_{W,0} \\leftarrow WOTS.verify(pk_H, \\sigma_{w,0}, Q_{HORST})$$ 公钥共 $l\\ blocks$，形成 L-Tree，计算出根节点，结合 $Auth_{A_0}$ 计算出 Tree 的根节点。 对于 layer 1~d-1，也是类似的操作，最后得出最上面的那棵树的根节点 $ROOT_{d-1}$，如果 $PK_1 = ROOT_{d-1}$，那么返回 true，否则返回 false。 图示：","link":"/2022/01/15/cryptography/signature/sphincs/"},{"title":"WOTS+","text":"本文建立在 WOTS 签名系统基础上，若还尚不熟悉该签名系统，可参考另一篇文章 Winternitz One Time Signature，论文见 W-OTS+ – Shorter Signatures for Hash-Based Signature Schemes，该算法产生的一对密钥只能签名一条消息。 下面按照 $(GEN, SIG, VER)$ 三元组描述该算法。 密钥生成($GEN$)$$(pk, sk) \\leftarrow GEN(1^k)$$ 首先，我们需要确定 $w$ 参数该参数决定哈希私钥多少次构造公钥，此外还需确定哈希之后摘要的长度 $m$ ，或者可以认为是采用的哈希函数族，$n$ 为其他哈希值长度。 一般情况下，我们要求：$w|m , w | 8$ ， 这将极大的简化之后的代码编写。 计算： $$ l_1 = \\lceil \\frac{m}{w}\\rceil,\\ l_2 = \\lfloor \\frac{\\log(l_1(2^w-1))}{w} \\rfloor + 1 ,\\ l = l_1 + l_2 $$ 定义 $c^i(x, \\boldsymbol{r})$ 运算： $$c^i(x,\\boldsymbol{r}) =\\begin{cases} &amp; x, &amp; i = 0 \\\\ &amp; \\mathcal{F}(c^{i-1}(x,\\boldsymbol{r})\\oplus r_i), &amp; i \\gt 0\\end{cases}$$ 其中$$\\begin{aligned}&amp; x \\in \\{0,1\\}^n\\\\ &amp; \\mathcal{F}: \\{0,1\\}^n \\rightarrow \\{0,1\\}^n\\\\ &amp; \\boldsymbol{r} = (r_1, r_2, \\dotsm, r_{2^w-1}) \\in \\{0,1\\}^{n\\times (2^w-1)}\\end{aligned}$$ 定义 $$\\boldsymbol{r}_{a,b} =\\begin{cases} &amp;(r_a, r_{a+1}, \\dotsm, r_b), &amp; a \\ge b \\\\ &amp; \\emptyset , &amp; a &lt; b\\end{cases}$$ 注：原文中使用 $c^i_k(x,\\boldsymbol{r})$ 含有参数 $k$，用来确定使用的哈希函数，这里为了简单和方便理解，不再暴露该参数。在实现中，通常我们会指定特定的哈希函数，不会使用参数 $k$。 通过 $PRG$ 生成 $l+2^w-1$ 个 $n$ bits 的随机数，前 $l$ 个随机数即为私钥： $$sk = (sk_0, sk_1, \\dotsm, sk_{l-1})\\tag{1}$$ 后 $2^w-1$ 个为掩码 $$\\boldsymbol{r} = (r_1, r_2, \\dotsm, r_{2^w-1})$$ 公钥包括 $l+1$ 个 $blocks$，第一个 $block$ 为掩码 $\\boldsymbol{r}$。后 $l$ 个通过 $sk$ 进行转换： $$pk_i = c^{2^w-1}(sk_{i-1},\\boldsymbol{r}), i \\in [1,l]$$ 最终，公钥为 $$\\begin{aligned} pk &amp;= (pk_0, pk_1, \\dotsm, pk_l) \\\\ &amp;= (\\boldsymbol{r}, c^{2^w-1}(sk_0, \\boldsymbol{r}), \\dotsm, c^{2^w-1}(sk_{l-1}, \\boldsymbol{r}))\\end{aligned}\\tag{2}$$ 图示： 消息签名($SIG$)$$ \\sigma(M) \\leftarrow SIG(M, sk)$$ 使用哈希函数 $$\\mathcal{H}: \\{0, 1\\}^* \\rightarrow \\{0,1 \\}^m$$ 计算消息 $M \\in \\{0, 1\\}^*$ 的摘要 $m \\in \\{0,1\\}^m$，这 $m$ bits 分成 $l_1$ 份，每一份 $w$ bits。 $$m = (m_0,m_1,\\dotsm,m_{l_1-1})$$ 上述的每 $w$ bits 表示的 $m_i, i \\in [0, l_1-1]$ 等价于一个整数。 接下来计算校验和(Checksum) $$C = \\sum^{l_1}_{i=1}(2^w-1-m_i) \\le l_1(2^w-1)$$ 将 $C$ 同样表示每部分 $w$ bits，或者说转换成以 $w$ 为基的数字。 $$c = (c_0, c_1, \\dotsm, c_{l_2-1})$$ 设$$b = (b_0, b_1, \\dotsm, b_{l-1}) = m || c$$ 即 $b$ 为 $m$ 和 $c$ 的串接。 则签名 $$\\begin{aligned} \\sigma &amp;= (\\sigma_0, \\sigma_1, \\dotsm, \\sigma_{l-1}) \\\\&amp;= (\\mathcal{F}^{b_0}(sk_0, \\boldsymbol{r}), \\mathcal{F}^{b_1}(sk_1,\\boldsymbol{r}), \\dotsm, \\mathcal{F}^{b_{l-1}}(sk_{l-1},\\boldsymbol{r}))\\end{aligned}$$ 图示： 消息校验($VER$)$$ false/true \\leftarrow VER(M, \\sigma(M), pk) $$ 通过上述同样的方式将消息 $M$ 转换成 $$b = (b_0, b_1, \\dotsm, b_{l-1})$$ 将传递过来的签名$$\\sigma = (\\sigma_0, \\sigma_1, \\dotsm, \\sigma_{l-1})$$ 进行如下处理，得到 $pk^\\prime$ $$\\begin{aligned} pk^\\prime &amp;= (\\boldsymbol{r}, pk^\\prime_1, pk^\\prime_2, \\dotsm, pk ^\\prime_{l}) \\\\ &amp;= (\\boldsymbol{r}, \\mathcal{F}^{2^w-1-b_0}(\\sigma_0), \\mathcal{F}^{2^w-1-b_1}(\\sigma_1), \\dotsm,\\mathcal{F}^{2^w-1-b_{l-1}}(\\sigma_{l-1}))\\end{aligned}$$ 和公钥 $$pk = (\\boldsymbol{r},pk_1, pk_2, \\dotsm, pk_{l})$$ 进行匹配，如果 $pk = pk^\\prime$ 则校验通过，返回 $true$，否则返回 $false$。 图示：","link":"/2021/12/28/cryptography/signature/wots-plus/"},{"title":"Winternitz One Time Signature","text":"1979 年 Ralph C. Merkle 提出了 Winternitz-OTS，其中运用了哈希链(hash chain)的结构，论文见 A Certified Digital Signature ，更加详细的算法描述见 Hash-based Digital Signature Schemes，该算法产生的一对密钥也只能签名一条消息。 下面按照 $(GEN, SIG, VER)$ 三元组描述该算法。 密钥生成($GEN$)$$(pk, sk) \\leftarrow GEN(1^k)$$ 首先，我们需要确定 $w$ 参数该参数决定哈希私钥多少次构造公钥，此外还需确定哈希之后摘要的长度 $n$ ，或者可以认为是采用的哈希函数族。 一般情况下，我们要求：$w|n , w | 8$ ， 这将极大的简化之后的代码编写。 计算： $$ l_1 = \\lceil \\frac{n}{w}\\rceil,\\ l_2 = \\lfloor \\frac{\\log(l_1(2^w-1))}{w} \\rfloor + 1 ,\\ l = l_1 + l_2 $$ 注: 这里采用 SPHINCS 中的计算公式，结果一致 通过 $PRG$ 生成 $l$ 个 $n$ bits 的随机数，即为私钥： $$sk = (sk_0, sk_1, \\dotsm, sk_{l-1})$$ 对 $l$ 个随机数哈希 $2^w-1$ 次 $$pk_i = hash^{2^w-1}(sk_i)$$ 组合即为公钥： $$pk = (pk_0, pk_1, \\dotsm, pk_{l-1})$$ 图示： 消息签名($SIG$)$$ \\sigma(M) \\leftarrow SIG(M, sk)$$ 计算消息 $M \\in \\{0, 1\\}^*$ 的摘要 $m \\in \\{0,1\\}^n$，这 $n$ bits 进行填充，使其能够被 $w$ 整除，分成 $l_1$ 份，每一份 $w$ bits。 因为上面我们提到，要求 $w\\mid n$，所以我们也可以认为，每一份 $w$ bits, 共 $l_1 = \\frac{n}{w}$ 份。 $$m = (m_0,m_1,\\dotsm,m_{l_1-1})$$ 上述的每 $w$ bits 表示的 $m_i, i \\in [0, l_1-1]$ 都可以表示一个整数，比如说 $w$ 为 4，$m_0$ 为 $1001_2$，则其表示 $9_{10}$ 接下来计算校验和(Checksum) $$C = \\sum^{l_1}_{i=1}(2^w-1-m_i) \\le l_1(2^w-1)$$ 将 $C$ 同样表示每部分 $w$ bits，或者说转换成以 $w$ 为基的数字。 $$c = (c_0, c_1, \\dotsm, c_{l_2-1})$$ 设$$b = (b_0, b_1, \\dotsm, b_{l-1}) = m || c$$ 即 $b$ 为 $m$ 和 $c$ 的串接。 则签名 $$\\begin{aligned} \\sigma &amp;= (\\sigma_0, \\sigma_1, \\dotsm, \\sigma_{l-1}) \\\\&amp;= (hash^{b_0}(sk_0), hash^{b_1}(sk_1), \\dotsm, hash^{b_{l-1}}(sk_{l-1}))\\end{aligned}$$ 图示： 消息校验($VER$)$$ false/true \\leftarrow VER(M, \\sigma(M), pk) $$ 通过上述同样的方式将消息 $M$ 转换成 $$b = (b_0, b_1, \\dotsm, b_{l-1})$$ 将传递过来的签名$$\\sigma = (\\sigma_0, \\sigma_1, \\dotsm, \\sigma_{l-1})$$ 进行如下处理，得到 $pk^\\prime$ $$\\begin{aligned} pk^\\prime &amp;= (pk^\\prime_0, pk^\\prime_1, \\dotsm, pk ^\\prime_{l-1}) \\\\ &amp;= (hash^{2^w-1-b_0}(\\sigma_0), hash^{2^w-1-b_1}(\\sigma_1), \\dotsm,hash^{2^w-1-b_{l-1}}(\\sigma_{l-1}))\\end{aligned}$$ 和公钥 $$pk = (pk_0, pk_1, \\dotsm, pk_{l-1})$$ 进行匹配，如果 $pk = pk^\\prime$ 则校验通过，返回 $true$，否则返回 $false$ 图示：","link":"/2021/12/24/cryptography/signature/wots/"},{"title":"揭露切片的真实面目","text":"本节主要内容位于 runtime/slice.go, 基于 go1.16.4/amd64 创建一个切片十分简单，如下即可创建一个长度为 10 的 int 切片 1slice := make([]int, 10) 但是具体是如何实现的呢，我们慢慢来看。 slice 结构在 slice.go 中首先定义的就是 slice 结构体 12345type slice struct { array unsafe.Pointer len int cap int} 这样一看，其实 slice 也不是那么神奇，结构体中包含了一个指向实际数据的指针array，以及切片的长度 len 和切片的容量 cap。 切片作为参数进行传递的时候，传递的不是整个结构体，实际上是传入了 slice 中的 array 数据指针，如果在函数内部修改了切片，那么同样会修改外部的切片。 切片的创建对于切片的创建，我们可以使用 make 关键字，在声明长度的时候，也可以同时声明容量 1slice := make([]int, 10, 10) 当我们使用 make 创建一个新的切片的时候，系统会调用 makeslice 函数为我们创建一个切片。 1func makeslice(et *_type, len, cap int) unsafe.Pointer 首先会计算需要分配的空间大小，如果分配的时候存在下面的任意一个问题，那么直接panic 分配的空间大小溢出 分配的空间大小大于最大的分配大小 maxAlloc 长度 len &lt; 0 长度 len 大于容量 cap 1mem, overflow := math.MulUintptr(et.size, uintptr(cap)) 然后最终调用 mallocgc 进行内存的分配 1mallocgc(mem, et, true) 这里的 mem 表示分配的空间大小，et 表示分配的类型，不同的类型会有不同的字节大小，最后一个参数 needzero=true 表示需要将申请的空间数据清零，也就是相当于设置了零值 append 扩容机制当我们调用 append 往切片中添加元素的时候，如果切片的容量 cap 不够了，会触发一次扩容，调用底层的 growslice 方法 1func growslice(et *_type, old slice, cap int) slice 当之前的容量小于 1024，那么会扩容的时候会容量会 double，如果否则的话，新的容量是原来的 1.25 倍 1234567891011121314151617181920newcap := old.capdoublecap := newcap + newcapif cap &gt; doublecap { newcap = cap} else { if old.cap &lt; 1024 { newcap = doublecap } else { // Check 0 &lt; newcap to detect overflow // and prevent an infinite loop. for 0 &lt; newcap &amp;&amp; newcap &lt; cap { newcap += newcap / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap &lt;= 0 { newcap = cap } }} 确定了容量大小之后，会判断容量是否溢出或者大于最大的分配空间大小，如果满足分配条件，那么会调用 mallocgc 进行内存的分配，然后调用 memmove 将原来的数据复制到分配的空间中 1memmove(p, old.array, lenmem) 最后返回一个新的切片，这个切片的数据和之前的数据是一样的，但是所在的空间不同，同时容量设置成新的数值 newcap。 1return slice{p, old.len, newcap} 我们可以使用下面的代码进行测试 12345slice := make([]int, 3, 4)slice = append(slice, 1)fmt.Printf(&quot;没有扩容之前的slice地址:%p, 长度:%d, 容量:%d\\n&quot;, slice, len(slice), cap(slice))slice = append(slice, 2)fmt.Printf(&quot;扩容之后的slice地址:%p, 长度:%d, 容量:%d\\n&quot;, slice, len(slice), cap(slice)) 输出 12没有扩容之前的slice地址:0xc0000121a0, 长度:4, 容量:4扩容之后的slice地址:0xc00000e340, 长度:5, 容量:8 所以在我们使用一个切片的时候尤其要注意添加元素之后是否进行了扩容，如果进行了扩容，那么前后两个切片对应的完全不是同一个切片了，这也是 growslice 会返回一个 slice 类型的原因。如果有另一个变量引用了当前切片变量，当没有进行扩容的时候，这两个切片指向的是同一个地址，如果进行了扩容，那么两个切片指向的地址不同，对元素的修改对于另外一个切片不可见 123456789101112131415// 引用修改a := make([]int, 2, 2)// b 引用 ab := a b[0] = 1fmt.Printf(&quot;a: %v, b: %v\\n&quot;, a, b)// 输出// a: [1 0], b: [1 0]//----------------------------------// 扩容之后不可见b = append(b, 1)b[1] = 10fmt.Printf(&quot;a: %v, b: %v\\n&quot;, a, b)// 输出// a: [1 0], b: [1 10 1] 因为 a，b 指向的是 同一块地址空间 ，所以修改 b，同时会影响 a。 在测试的时候有一个很有趣的想法，比如说下面的代码，输出的结果其实还是很简单的因为 a 的长度为 1，b 修改的索引在 a 中访问不到，所以输出为：a: [0], b: [0 2] 12345a := make([]int, 1, 2)b := a b = append(b, 1)b[1] = 2fmt.Printf(&quot;a: %v, b: %v\\n&quot;, a, b) 这里之所以出现这种情况，因为 b:=a 的时候，执行的是值拷贝，a,b 两个指针也是一样的，指向同样的数据，其它的两个字段 len, cap 也是一样的值拷贝，但是这两个双方修改起来都是不可见的，它们的地址不同。 之所以上面的输出为 a: [0], b: [0 2]，并不是说 a 对应的数据空间中的数据没有修改，而只是字段长度 len 限制了访问的内存，那么我们有一个很自然的想法，修改 a 对应的长度，那么是不是就可以就可以访问到修改的区域数据，那么肯定要试试啊，使用 unsafe 包修改一下底层数据 12(*reflect.SliceHeader)(unsafe.Pointer(&amp;a)).Len = 2fmt.Printf(&quot;a: %v, b: %v\\n&quot;, a, b) 执行上面的代码，yeah，我们得到了想要的输出 a: [0 2], b: [0 2]当然，这些都是一些 黑魔法，平时不建议使用 切片的复制我们知道切片的传递类似于引用(准确的来说，Go中只有值传递，只不过传递的是指针，我们可以认为是引用传递)，所以在使用的时候我们不想修改原来切片中的数据，那么可以先使用 copy 复制一份，然后在新的切片中进行修改。 当我们调用 copy 的时候，底层一般情况下使用的是 makeslicecopy 函数 1func makeslicecopy(et *_type, tolen int, fromlen int, from unsafe.Pointer) unsafe.Pointer 这个函数其实也很简单，主要分为三步： 1.计算需要分配的空间大小 1234567891011if uintptr(tolen) &gt; uintptr(fromlen) { var overflow bool tomem, overflow = math.MulUintptr(et.size, uintptr(tolen)) if overflow || tomem &gt; maxAlloc || tolen &lt; 0 { panicmakeslicelen() } copymem = et.size * uintptr(fromlen) } else { tomem = et.size * uintptr(tolen) copymem = tomem } 2.使用 mallocgc 进行内存的数据分配 123456789101112var to unsafe.Pointer if et.ptrdata == 0 { to = mallocgc(tomem, nil, false) if copymem &lt; tomem { memclrNoHeapPointers(add(to, copymem), tomem-copymem) } } else { to = mallocgc(tomem, et, true) if copymem &gt; 0 &amp;&amp; writeBarrier.enabled { bulkBarrierPreWriteSrcOnly(uintptr(to), uintptr(from), copymem) } } 3.使用 memmove 复制原来的数据到新的内存空间中 1memmove(to, from, copymem) 性能优化从上面的分析我们知道，当使用 append 函数添加元素的时候，如果切片的容量 cap 不够，那么就需要进行扩容处理，期间会调用 memmove 进行一次拷贝，一定程度上会影响性能。如果我们知道确切的大小，使用下标进行赋值最理想，我们如果大概知道容量，那么可以采取预分配的方式，先分配一定大容量的切片，从而可以减少切片扩容的次数。 下面我们用实际的测试来说明 123456789101112131415161718192021222324252627282930313233343536373839func makeSlice1() { slice := make([]int, 1024) for i := 0; i &lt; 1024; i++ { slice[i] = i }}func makeSlice2() { slice := make([]int, 0) for i := 0; i &lt; 1024; i++ { slice = append(slice, i) }}func makeSlice3() { slice := make([]int, 0, 1024) for i := 0; i &lt; 1024; i++ { slice = append(slice, i) }}func BenchmarkMakeSlice(b *testing.B) { b.Run(&quot;下标&quot;, func(b *testing.B) { for i := 0; i &lt; b.N; i++ { makeSlice1() } }) b.Run(&quot;原始&quot;, func(b *testing.B) { for i := 0; i &lt; b.N; i++ { makeSlice2() } }) b.Run(&quot;预分配&quot;, func(b *testing.B) { for i := 0; i &lt; b.N; i++ { makeSlice3() } })} 运行基准测试，我们可以得到下面的结果，很明显可以看出性能差异来 123456goos: linuxgoarch: amd64cpu: Intel(R) Core(TM) i7-8565U CPU @ 1.80GHzBenchmarkMakeSlice/下标-8 3401653 668.9 ns/opBenchmarkMakeSlice/原始-8 127171 8066 ns/opBenchmarkMakeSlice/预分配-8 704804 1620 ns/op 所以在业务中，选择 下标 &gt; 预分配容量大小 &gt; 0容量","link":"/2021/06/28/golang/source-code/slice/"},{"title":"字符串相加","text":"本节内容主要代码位于 runtime/string.go 以下面的代码进行说明 123a := &quot;Hello &quot;b := &quot;World &quot;_ = a + b 当执行 a+b 的时候，底层上调用的是 concatstrings，并且将a, b 两个操作数作为 slice 传递进去，如下： tmpBuf 是一个预定义的 tmpStringBufSize 大小的 byte 数组。 12const tmpStringBufSize = 32type tmpBuf [tmpStringBufSize]byte 函数中首先定义了三个变量，idx 表示当前遍历的字符串索引号，l 表示当前拼接得到的字符串长度，count 表示长度不为0的字符串个数。 123idx := 0l := 0count := 0 然后我们遍历每一个字符串，通过 len 函数获取到它的长度，如果长度为0，执行 continue，不需要进行其他操作，由于拼接之后的字符串长度可能过长，导致整数溢出，所以源码中使用 l+n&lt;l 加以判断，当溢出的时候抛出错误。执行这个遍历之后，我们获取到了最终的字符串长度。 123456789101112for i, x := range a { n := len(x) if n == 0 { continue } if l+n &lt; l { throw(&quot;string concatenation too long&quot;) } l += n count++ idx = i } 遍历完成之后，如果 count 为0，说明并没有实在意义的字符串，直接返回空字符串，如果 count 为1，那么我们直接返回对应的字符串就好，无需进行拼接，这个时候，count 为1，那么必然满足条件的这个字符串的索引号为 idx，所以直接返回 a[idx]。 1234567if count == 0 { return &quot;&quot;}if count == 1 &amp;&amp; (buf != nil || !stringDataOnStack(a[idx])) { return a[idx]} 不过这里除了需要判断 count==1 之外，我们还需要判断 a[idx] 是否还在栈上，或者 buf 是否从调用帧逃逸出去了，这样我们才能够直接返回。 这里的 stringDataOnStack 其实还挺好玩的，代码如下: 12345func stringDataOnStack(s string) bool { ptr := uintptr(stringStructOf(&amp;s).str) stk := getg().stack return stk.lo &lt;= ptr &amp;&amp; ptr &lt; stk.hi} 我们知道其实一个字符串就是一个 stringStruct 结构体，str 指向底层保存的数据，len 保存长度，在运行的每一个 goroutine 都分配了自己的栈空间，我们只需要判断这个地址是否在栈范围之内就好，也就是 stk.lo &lt;= ptr &amp;&amp; ptr &lt; stk.hi 。 1234type stringStruct struct { str unsafe.Pointer len int} 如果上面的条件都不满足，那么我们需要进行下一步操作 12345s, b := rawstringtmp(buf, l)for _, x := range a { copy(b, x) b = b[len(x):]} 首先调用 rawstringtmp 返回一个 string 类型的 s 和一个 []byte 的 b，其中 s 的底层数据就是 b，修改 b，那么 s 就会被同步修改。循环中其实在不断的将需要拼接的单个字符串摆放在对应的位置 实际上，根据不同的字符串拼接个数，定义了不同的函数，但是本质上都是调用上述函数 123456789101112131415func concatstring2(buf *tmpBuf, a [2]string) string { return concatstrings(buf, a[:])}func concatstring3(buf *tmpBuf, a [3]string) string { return concatstrings(buf, a[:])}func concatstring4(buf *tmpBuf, a [4]string) string { return concatstrings(buf, a[:])}func concatstring5(buf *tmpBuf, a [5]string) string { return concatstrings(buf, a[:])} 调试的时候发现，如果我们的拼接字符串个数在 [2,5] 范围内，那么我们会调用上述函数，然后再调用 concatstrings，如果超过这个值呢，会直接调用 concatstrings 进行处理。","link":"/2021/06/26/golang/source-code/string-add/"},{"title":"字符串与切片转换","text":"本节内容主要代码位于 runtime/string.go， 基于 go1.16.4/amd64 以下面的代码进行说明 123slice := []byte{'h', 'e', 'l', 'l', 'o'}slice2str := string(slice)str2slice := []byte(slice2str) slice to string底层将 byte slice 转换成 string 的函数为 slicebytetostring。 1func slicebytetostring(buf *tmpBuf, ptr *byte, n int) (str string) 这个函数接收三个参数，*tmpBuf 是一个指向 byte 数组的指针，ptr 指向 slice 第一个元素的地址， n 表示切片的长度。 函数执行的时候，首先会判断字符串的长度 n 是否为0，如果为0的话，直接返回空字符串。 如果切片的长度为 1 的话，那么会直接从一个 全局的静态表 中取出对应的数据，并且获取到指向这个元素指针，然后通过字符串的底层结构 stringStruct 进行赋值即可 123456789if n == 1 { p := unsafe.Pointer(&amp;staticuint64s[*ptr]) if sys.BigEndian { p = add(p, 7) } stringStructOf(&amp;str).str = p stringStructOf(&amp;str).len = 1 return } staticuint64s，位于 runtime/iface.go 是一个静态数组，可以避免给一些比较小的整数值分配空间 12345678var staticuint64s = [...]uint64{ 0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f, ..... 0xf0, 0xf1, 0xf2, 0xf3, 0xf4, 0xf5, 0xf6, 0xf7, 0xf8, 0xf9, 0xfa, 0xfb, 0xfc, 0xfd, 0xfe, 0xff,} 那么我们可以知道，对于长度为1个且值相同的 byte slice，那么进行 string 转换的时候，对应的地址都是一样的，测试代码如下 123456test := []byte{'1'}test2 := []byte{'1'}a := string(test)b := string(test2)fmt.Printf(&quot;%x\\n&quot;, (*reflect.StringHeader)(unsafe.Pointer(&amp;a)).Data)fmt.Printf(&quot;%x\\n&quot;,(*reflect.StringHeader)(unsafe.Pointer(&amp;b)).Data) 本地测试的时候输出均为 3e0a08，也就验证了我们的想法 如果切片的长度大于1，那么首先优先使用 buf 作为底层的数组，如果长度不够的话，则使用 mallocgc 分配大小为 n 的空间 12345if buf != nil &amp;&amp; n &lt;= len(buf) { p = unsafe.Pointer(buf)} else { p = mallocgc(uintptr(n), nil, false)} 分配好空间之后，进行赋值操作，最后调用 memmove 函数将 ptr 指向的n个字节的数据复制到申请的 p 中 123stringStructOf(&amp;str).str = pstringStructOf(&amp;str).len = nmemmove(p, unsafe.Pointer(ptr), uintptr(n)) 所以主要的处理流程如下 string to slicestringtoslicebyte 负责将 string 类型转换成 byte slice 类型 1func stringtoslicebyte(buf *tmpBuf, s string) []byte 如果 string 的长度小于 buf 的长度，同时 buf 不为空，那么我们使用 buf 作为切片的存放空间，否则，我们需要调用 rawbyteslice 一块 len(s) 大小的 byte 切片大小空间，最后将s中的值复制到空间中返回这一块数据 12345678var b []byteif buf != nil &amp;&amp; len(s) &lt;= len(buf) { *buf = tmpBuf{} b = buf[:len(s)]} else { b = rawbyteslice(len(s))}copy(b, s) rawbyteslice 其实也是调用 mallocgc 分配空间的，所以其实两者的转换在本质上都是申请一个空间，然后将数据拷贝一下，也没有什么特别神奇的地方，操作不同的具体原因就是 byte slice 和 string 类型的表示不太一样。 123456789101112// string的底层结构type stringStruct struct { str unsafe.Pointer len int}// slice 底层结构type slice struct { array unsafe.Pointer len int cap int} 当将 stringStruct 转换成 slice 的时候，我们需要将 str 指向的数据拷贝到 array 中当将 slice 转换成 stringStruct 的时候，我们需要将 slice中array 指向的数据拷贝到 str 中 性能优化通过上面的分析，我们知道在进行 byte slice 和 string 的转换的时候是会需要进行复制的，这个代价很大，需要重新分配空间，那么如果业务中我们对于一个 string 或者 byte slice 只需要进行读取操作，不要进行修改数据，那么可以通过强转进行实现，下面对每种转换进行了基准测试 1234567891011121314151617181920212223242526272829303132333435363738394041func BenchmarkByte2String1(b *testing.B) { bytes := []byte{'h', 'e', 'l', 'l', 'o'} for i := 0; i &lt; b.N; i++ { _ = string(bytes) }}func BenchmarkByte2String2(b *testing.B) { bytes := []byte{'h', 'e', 'l', 'l', 'o'} var s string for i := 0; i &lt; b.N; i++ { (*reflect.StringHeader)(unsafe.Pointer(&amp;s)).Data = (*reflect.SliceHeader)(unsafe.Pointer(&amp;bytes)).Data (*reflect.StringHeader)(unsafe.Pointer(&amp;s)).Len = len(bytes) // s = *(*string)(unsafe.Pointer(&amp;bytes)) // 也可 } // b.Logf(&quot;%s&quot;, s)}func BenchmarkString2Byte1(b *testing.B) { str := &quot;hello&quot; for i := 0; i &lt; b.N; i++ { _ = []byte(str) }}func BenchmarkString2Byte2(b *testing.B) { str := &quot;hello&quot; var bytes []byte for i := 0; i &lt; b.N; i++ { (*reflect.SliceHeader)(unsafe.Pointer(&amp;bytes)).Data = (*reflect.StringHeader)(unsafe.Pointer(&amp;str)).Data (*reflect.SliceHeader)(unsafe.Pointer(&amp;bytes)).Len = len(str) (*reflect.SliceHeader)(unsafe.Pointer(&amp;bytes)).Cap = len(str) // bytes = *(*[]byte)(unsafe.Pointer(&amp;str)) // 也可 } // b.Logf(&quot;%s&quot;, bytes)} 运行 go test -bench . convert_test.go 结果 1234567goos: linuxgoarch: amd64cpu: Intel(R) Core(TM) i7-8565U CPU @ 1.80GHzBenchmarkByte2String1-8 314203983 3.847 ns/opBenchmarkByte2String2-8 1000000000 0.2579 ns/opBenchmarkString2Byte1-8 225171763 5.340 ns/opBenchmarkString2Byte2-8 1000000000 0.2513 ns/op 由此可见，使用底层进行转换能够提高10多倍性能，在不需要修改数据且对性能要求很高的情况下，可以考虑使用该种转换形式。","link":"/2021/06/27/golang/source-code/string-bytes/"},{"title":"国内 Wireshark 安装","text":"wireshark 是一款网络协议分析器，使用这个工具能够让我们深层次的观察到网络数据传输过程中发生的一切，包括 ICMP、TCP、UDP、SSL 等协议的细节，但是因为官方下载资源地址为国外，在国内它的下载速度十分慢，虽然资源包大小不大，但是动不动得下载好几个小时。 不过好在国内有对应的下载镜像，比如清华大学的下载镜像源：https://mirrors.tuna.tsinghua.edu.cn/wireshark/ 在这里可以快速的下载 Mac 和 Windows 版本的应用，暂时不提供 Unix 类系统的下载资源（官方也未提供），但是可以通过 sudo apt install wireshark 之类的命令直接下载。 下载完成打开之后，选择对应的网络接口 (Mac 上按住 Command 可以选择多个网络接口)，然后便可以进行网络数据抓包：","link":"/2024/12/15/network/tool/wireshark-install/"},{"title":"Golang 报错 package xxx is not in GOROOT or GOPATH 或者 cannot find package “xxx“ in any of","text":"GO111MODULE=”off”在 GO111MODULE=&quot;off&quot; 的条件下，并且写的代码不在 $GOPATH/src 下，也就是说下面的 main.go 不在$GOPATH/src 目录下面，同时我想要使用当前项目目录下的另一个 module 里面的可导出标识符(函数、变量等)，但是这个模块 不是标准库，或者说不在 GOROOT 里(一般我们不会修改 GOROOT 中的内容) 运行代码会报错 123main.go:4:2: cannot find package &quot;module&quot; in any of: /usr/local/go/src/module (from $GOROOT) /home/linux/go/src/module (from $GOPATH) 解决方案: 设置 GOPATH12go env -w GOPATH=~/test# 这里设置为 ~/test是因为我需要在~/test/src下面写代码 然后在 $GOPATH/src 目录下写 go 的编译器会在 $GOPATH/src 下面寻找对应的模块，src 下的每一个目录都可以对应一个模块，目录中的目录也可以是一个模块，如果，我们需要访问一个嵌套目录中的模块，比如下图 我们需要调用 module 目录中的 moduleA 模块，只需要使用 1import &quot;module/moduleA&quot; 动图演示 GO111MODULE=”on”在 GO111MODULE=&quot;on&quot; 的条件下，我们直接调用写好的模块，如下图所示 会直接报错 1main.go:4:2: package module is not in GOROOT (/usr/local/go/src/module) 解决方案 第一种方式：设置 GO111MODULE=&quot;off&quot;，然后像上面的那种方式一样设置 GOPATH 第二种方式：使用 go mod，如下 首先我们需要初始化一个 go.mod ，使用 12go mod init test# test可以是任意的名称 然后我们引入模块的时候，以 test (初始化时定义)开头，然后接模块路径，比如 1import &quot;test/module&quot; 如果使用这种方式 Goland 报错，但是可以进行正常编译，那么可以删除当前目录下的 .idea 目录然后重启项目即可 如果我们想要引用嵌套模块也是一样的 1import &quot;test/module/moduleA&quot; 动图演示","link":"/2020/07/11/golang/fundamental/import-package/"},{"title":"cargo 中的语义版本控制系统","text":"Cargo 和其他很多工具一样，遵循语义版本控制系统，程序库以 $$\\text{major.minor.patch}$$ 格式指定，其中 major：项目进行重大更新，且包含不向后兼容的功能 minor：仅在以向后兼容的方式添加新功能 patch：仅在以向后兼容的方式修复错误，并且没有添加任何新的功能 在 Cargo.toml 中，当我们指定了完整的版本号，如 1.0.0，那么 cargo 会帮我们拉取对应版本号的代码； 1234[dependencies]serde = &quot;1.0.0&quot;# 或者# serde = { version = &quot;1.0.0&quot; } 当我们不关心补丁版本号时，如指定 1.0，那么 cargo 会帮我们拉取 1.0.* 的最高版本号对应的库，当我们只关心主版本号时，如指定版本号为 1，那么 cargo 会拉取主版本号为 1 对应的最新库。 虽然我们也可以直接指定最新的版本，如下，但是由于版本之间可能不兼容，所以强烈不推荐，并且 Crates.io 不允许上传具有通配符依赖关系的包。 12[dependencies]serde = &quot;*&quot; 我们也可以指定最低的版本号： 12[dependencies]serde = &quot;^1.1.0&quot; 上面的这种书写方式表示，使用最新的 1.*.* 的版本号，并且版本号至少为 1.1.0。 以下是一些例子： 12345678^1.2.3 := &gt;=1.2.3 &lt;2.0.0^1.2 := &gt;=1.2.0 &lt;2.0.0^1 := &gt;=1.0.0 &lt;2.0.0^0.2.3 := &gt;=0.2.3 &lt;0.3.0^0.2 := &gt;= 0.2.0 &lt; 0.3.0^0.0.3 := &gt;=0.0.3 &lt;0.0.4^0.0 := &gt;=0.0.0 &lt;0.1.0^0 := &gt;=0.0.0 &lt;1.0.0 我们也可以指定更新最小版本，如果指定 major 版本，minor 版本和 patch 程序版本，或仅指定 major 版本和 minor 版本，则仅允许 patch 程序级别更改。如果仅指定 major 版本，则允许进行 minor 和 patch 级别更改。 12[dependencies]serde = &quot;~1.1.0&quot; 意思是我们可以使用版本 &gt;=1.1.0 &amp;&amp; &lt; 2.0.0 的代码库。 一些例子： 123~1.2.3 := &gt;=1.2.3 &lt;1.3.0~1.2 := &gt;=1.2.0 &lt;1.3.0~1 := &gt;=1.0.0 &lt;2.0.0 因为我们也可能直接从 GitHub 等网站上拉取代码库，所以我们可以直接指定 此外，cargo 还有很多其他的工具链可以下载，比如 cargo-edit 提供了 4 条命令：cargo add、cargo rm、cargo edit、cargo upgrade，使用 cargo install cargo-edit 即可下载安装这 4 条命令。 cargo add 时，我们可以直接指定需要的版本号，比如： 1cargo add serde@1.0.0 更多详细内容可见 指定依赖","link":"/2022/01/04/rust/cargo/version/"},{"title":"Rust 编写 derive 宏","text":"宏可以帮助我们减少重复代码的编写，在 Rust 中有两种宏定义， 声明宏 过程宏 其中，声明宏只是简单的 token 替换，我们无法知道代码结构中的其他信息，过程宏可以获取更加详细的数据，比如我们可以获取结构体中字段的名称，类型等等。 下面介绍如何编写一个简单的 derive 过程宏。 首先对于过程宏来说，不能和引用的 crate 放置在同一个 crate 中，需要单独放置在一个 crate 中，同时我们需要在 Cargo.toml 中配置 12[lib]proc-macro = true 对于 derive 宏而言，即为输入为 TokenStream，输出也是 TokenStream 的函数，且输出的内容会 append 到代码中，而不会覆盖输入的内容 1fn proc_marco(input: TokenStream) -&gt; TokenStream; 我们使用 syn 对输入进行解析，获取其中的 token 信息，使用 quote 构造输出 123[dependencies]quote = &quot;1&quot;syn = &quot;1.0&quot; 使用 proc_macro_derive 来标注这个函数是一个 derive 宏，同时也可以指定 attributes 用于指定可以在宏范围中使用的 attr，可以指定多个使用逗号进行分割。 12#[proc_macro_derive(PrintField, attributes(field, typ))]pub fn print_field(input: TokenStream) -&gt; TokenStream; 如果要使用该宏，使用方式如下 123456#[derive(PrintField)]struct Server { #[field=&quot;hi&quot;] host: String, port: u16 } derive 宏的具体编写步骤主要分成以下三步 使用 syn 提供的方法解析输入 12345#[proc_macro_derive(PrintField, attributes(field, typ))]pub fn print_field(input: TokenStream) -&gt; TokenStream { let DeriveInput { ident, data, .. } = parse_macro_input!(input as DeriveInput); //....} 从输入中获取到需要的信息，进行保存 123456789101112131415161718192021222324#[proc_macro_derive(PrintField, attributes(field, typ))]pub fn print_field(input: TokenStream) -&gt; TokenStream { let DeriveInput { ident, data, .. } = parse_macro_input!(input as DeriveInput); let mut field_names = vec![]; if let syn::Data::Struct(s) = data { if let syn::Fields::Named(f) = s.fields { // 获取所有的字段名字，最后进行打印 for field in f.named.iter() { field_names.push( field .ident .as_ref() .map(|ident| ident.to_string()) .unwrap_or_default(), ); } } } println!(&quot;{:?}&quot;, field_names); //....} 通过 quote 构造输出 123456789101112131415161718192021222324252627282930#[proc_macro_derive(PrintField, attributes(field, typ))]pub fn print_field(input: TokenStream) -&gt; TokenStream { let DeriveInput { ident, data, .. } = parse_macro_input!(input as DeriveInput); let mut field_names = vec![]; if let syn::Data::Struct(s) = data { if let syn::Fields::Named(f) = s.fields { println!(&quot;{:?}&quot;, f.to_token_stream().to_string()); for field in f.named.iter() { field_names.push( field .ident .as_ref() .map(|ident| ident.to_string()) .unwrap_or_default(), ); } } } println!(&quot;{:?}&quot;, field_names); quote!( impl #ident { pub fn hello_world(&amp;self) { println!(&quot;Hello World&quot;) } } ).into()} 函数中的 Ident 类型变量，使用 #variable_name 的方式在 quote 中进行引用，不能直接使用字符串，可以通过下面的方式进行创建 123use proc_macro2::Span;use syn::Ident;Ident::new(&quot;fn_name&quot;, Span::call_site()); 如果需要进行循环迭代，可以使用 #()* 的方式表示 12345678// fn_name 可以理解为 Vec&lt;Ident&gt; 类型impl #ident { #( pub fn #fn_name() { println!(&quot;call {}&quot;, #fn_name.to_string()) } ) *} 因为宏是在编译的过程中进行处理的，所以即使我们宏中代码实现的不高效，不影响运行时性能。编写好的宏，可以使用 cargo expand 命令进行展开，如果提示没找到该命令，使用 cargo install cargo-expand 进行安装。 简单写了一个 derive 宏自动为结构体生成 setter 和 getter 方法，仓库：construct，可以学习参考。","link":"/2022/09/03/rust/macro/derive/"},{"title":"Gin 快速上手","text":"使用 Go 搭建 Web 后端，最简单的就是使用原生的 net/http 库了，下面是一个简单的例子: 123456789101112131415package mainimport ( &quot;fmt&quot; &quot;net/http&quot;)func greet(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, &quot;Hello World!&quot;)}func main() { http.HandleFunc(&quot;/&quot;, greet) http.ListenAndServe(&quot;:8080&quot;, nil)} 运行代码之后，便会在8080端口处监听，接收客户端的请求，然后返回响应，访问 http://localhost:8080 即可以访问到对应的内容。 对于简单的服务器，使用标准库提供的函数就已经可以解决了，如果更加复杂一点，使用标准库处理就显得力不从心了，这时候我们可以使用网上提供的框架，比如 Gin。 Gin 使用起来特别方便(真的很方便！)，并且支持中间件，异常处理，路由组等等，下面用代码演示一下如何使用吧。 快速入门首先我们需要下载gin，我们要保证设置了 GO111MODULE=on，然后进行下载 12go mod init gin-demogo get -u github.com/gin-gonic/gin 下面是一个简单的web服务，我们使用 gin.Default() 创建一个默认的路由引擎,然后使用 GET 方式注册一个路由，负责接受来自客户端的 GET 请求，并且返回一个 JSON 数据给客户端。 12345678910111213package mainimport &quot;github.com/gin-gonic/gin&quot;func main() { router := gin.Default() router.GET(&quot;/ping&quot;, func(c *gin.Context) { c.JSON(200, gin.H{ &quot;message&quot;: &quot;pong&quot;, }) }) r.Run() // 监听并在 0.0.0.0:8080 上启动服务} 使用curl进行测试结果如下，符合预期 12$ curl http://localhost:8080/ping {&quot;message&quot;:&quot;pong&quot;} 使用HTTP方法12345678910// 使用默认中间件（logger 和 recovery 中间件）创建 gin 路由router := gin.Default()router.GET(&quot;/someGet&quot;, getting)router.POST(&quot;/somePost&quot;, posting)router.PUT(&quot;/somePut&quot;, putting)router.DELETE(&quot;/someDelete&quot;, deleting)router.PATCH(&quot;/somePatch&quot;, patching)router.HEAD(&quot;/someHead&quot;, head)router.OPTIONS(&quot;/someOptions&quot;, options) 使用路由组使用 Group 方法进行创建，路由组可以使得路由结构更加清晰 123456789router := gin.Default()// 简单的路由组: v1v1 := router.Group(&quot;/v1&quot;){ v1.POST(&quot;/login&quot;, loginEndpoint) v1.POST(&quot;/submit&quot;, submitEndpoint) v1.POST(&quot;/read&quot;, readEndpoint)} 返回内容返回JSON内容gin.Context 提供了 JSON 方法， 我们可以直接使用， 第一个参数为返回的状态码，第二个参数为返回的数据，gin.H 其实就是 map[string]interface{} 的简写，最后会被序列化返回给客户端。 123456router := gin.Default()router.GET(&quot;/ping&quot;, func(c *gin.Context) { c.JSON(200, gin.H{ &quot;message&quot;: &quot;pong&quot;, })}) 返回HTML内容首先需要加载 HTML 模板文件，然后使用 c.HTML 方法将数据注入到指定的模板文件中 1234567router := gin.Default()router.LoadHTMLGlob(&quot;templates/**/*&quot;)router.GET(&quot;/posts/index&quot;, func(c *gin.Context) { c.HTML(http.StatusOK, &quot;posts/index.tmpl&quot;, gin.H{ &quot;title&quot;: &quot;Posts&quot;, })}) 模板文件 posts/index.tmpl 1234567{{ define &quot;posts/index.tmpl&quot; }}&lt;html&gt;&lt;h1&gt; {{ .title }}&lt;/h1&gt;&lt;p&gt;Using posts/index.tmpl&lt;/p&gt;&lt;/html&gt;{{ end }} 返回字符串String方法即可 123router.GET(&quot;String&quot;, func(c *gin.Context) { c.String(http.StatusOK, &quot;Hello World&quot;)}) 获取查询参数对于简单的 query 参数，使用 GetQuery 和 DefaultQuery 方法就好了 12345678910111213router.GET(&quot;/query&quot;, func(c *gin.Context) { q, exist := c.GetQuery(&quot;q&quot;) if exist { c.JSON(200, gin.H{ &quot;data&quot;: q, }) } else { c.JSON(200, gin.H{ &quot;code&quot;: 400, &quot;message&quot;: &quot;q is not exist&quot;, }) }}) 12345$ curl http://localhost:8080/query{&quot;code&quot;:400,&quot;message&quot;:&quot;q is not exist&quot;}$ curl http://localhost:8080/query?q=hello{&quot;data&quot;:&quot;hello&quot;} 不过如果需要获取到的 query 参数是数组或者 map，那么就需要使用其他的方式了，比如数组需要使用GetQueryArray or QueryArray 和 GetQueryMap or QueryMap。 123456789101112131415161718192021222324252627r.GET(&quot;/query_array&quot;, func(c *gin.Context) { q, exist := c.GetQueryArray(&quot;q&quot;) if exist { c.JSON(200, gin.H{ &quot;data&quot;: q, }) } else { c.JSON(200, gin.H{ &quot;code&quot;: 400, &quot;message&quot;: &quot;q is not exist&quot;, }) }})r.GET(&quot;/query_map&quot;, func(c *gin.Context) { q, exist := c.GetQueryMap(&quot;q&quot;) if exist { c.JSON(200, gin.H{ &quot;data&quot;: q, }) } else { c.JSON(200, gin.H{ &quot;code&quot;: 400, &quot;message&quot;: &quot;q is not exist&quot;, }) }}) 12345$ curl &quot;http://localhost:8080/query_array?q=1&amp;q=2&quot;{&quot;data&quot;:[&quot;1&quot;,&quot;2&quot;]}$ curl &quot;http://localhost:8080/query_map?q[1]=1&amp;q[2]=2&quot;{&quot;data&quot;:{&quot;1&quot;:&quot;1&quot;,&quot;2&quot;:&quot;2&quot;}} 获取表单内容12345678910router.POST(&quot;/form&quot;, func(c *gin.Context) { name := c.PostForm(&quot;name&quot;) // or GetPostForm list := c.PostFormArray(&quot;list&quot;) // or GetPostFormArray m := c.PostFormMap(&quot;map&quot;) // or GetPostFormMap c.JSON(http.StatusOK, gin.H{ &quot;name&quot;: name, &quot;list&quot;: list, &quot;map&quot;: m, })}) 12$ curl --form name=test --form list=1 --form list=2 --form map[1]=1 --form map[2]=2 http://localhost:8080/form{&quot;list&quot;:[&quot;1&quot;,&quot;2&quot;],&quot;map&quot;:{&quot;1&quot;:&quot;1&quot;,&quot;2&quot;:&quot;2&quot;},&quot;name&quot;:&quot;test&quot;} 文件上传单文件12345678910111213141516router := gin.Default()// 为 multipart forms 设置较低的内存限制 (默认是 32 MiB)router.MaxMultipartMemory = 8 &lt;&lt; 20 // 8 MiBr.POST(&quot;/upload/single&quot;, func(c *gin.Context) { // 单文件 file, _ := c.FormFile(&quot;file&quot;) fmt.Println(file.Filename) // 上传文件至指定目录 c.SaveUploadedFile(file, dst) c.JSON(http.StatusOK, gin.H{ &quot;code&quot;: 0, &quot;message&quot;: &quot;upload success&quot;, })}) 多文件1234567891011router.POST(&quot;/upload/multiple&quot;, func(c *gin.Context) { // Multipart form form, _ := c.MultipartForm() files := form.File[&quot;upload[]&quot;] for _, file := range files { log.Println(file.Filename) // 上传文件至指定目录 c.SaveUploadedFile(file, dst) } 使用中间件中间件可以对经过一个路由的请求进行预处理，比如说鉴权，记录日志等等, gin 中创建一个中间件十分简单，我们只需要返回一个 gin.Handler 的类型就好了。下面就创建了一个简单的中间件来计算路由花费的时间 12345678func Cost() gin.HandlerFunc{ return func(c *gin.Context) { start := time.Now() // 一定要调用 c.Next() fmt.Println(&quot;total time: &quot;, time.Now().Sub(start)) }} 使用中间件有多种方式，一种是全局中间件，一种是路由组中间件，一种单个路由中间件 12345678910111213router := gin.New() // 没有使用任何中间件的路由引擎// 整个路由router.Use(Cost())// 路由组group := r.Group(&quot;/api&quot;)group.Use(Cost()){ group.GET(&quot;/greet&quot;, greet) group.POST(&quot;/form&quot;, form)}// 单个路由，不会影响其他路由router.POST(&quot;/middleware&quot;, Cost(), controller) 绑定参数至结构体在业务中，我们经常需要将用户发送过来的请求绑定到一个特定的结构体中，这样使用起来更加方便，gin 提供了大量的绑定方法，下面以 ShouldBind 为例讲解如何使用： 12345678910111213141516171819202122232425// 首先声明需要绑定参数的结构体type LoginReq struct { // form 参数指定需要绑定的表单内容 // binding:&quot;required&quot; 表示不能非空，否则绑定的时候返回错误 // 如果该字段非必须，可以去掉该tag Username string `form:&quot;username&quot; json:&quot;username&quot; binding:&quot;required&quot;` Password string `form:&quot;password&quot; json:&quot;password&quot; binding:&quot;required&quot;`}r.POST(&quot;/login&quot;, func(c *gin.Context) { var form LoginReq if err := c.ShouldBind(&amp;form); err != nil { c.JSON(http.StatusOK, gin.H{ &quot;code&quot;: -1, &quot;message&quot;: err.Error(), }) return } fmt.Printf(&quot;%#v\\n&quot;, form) c.JSON(http.StatusOK, gin.H{ &quot;code&quot;: 0, &quot;message&quot;: form, })}) gin 还有很多方面的特性，这里只是简单的介绍了如何去使用，更多的使用方法可以参考文档，个人之前也用过写了一个小项目。","link":"/2021/07/22/golang/framework/gin/basic/"},{"title":"Go 代码测试覆盖度","text":"在 Go 语言中，本身提供了一些命令来展示代码的覆盖度，我们可以通过执行这些命令来查看哪些语句被测试覆盖到了，哪些没有，进而可以编写更多的 case 来提高代码的覆盖度。 执行下面的命令，可以得出项目里面所有 package 的覆盖度 1go test ./... -cover 输出结果如下： 1234567? github.com/junhaideng/sphincs/api [no test files]? github.com/junhaideng/sphincs/cmd [no test files]ok github.com/junhaideng/sphincs/common 0.462s coverage: 51.9% of statements? github.com/junhaideng/sphincs/hash [no test files]ok github.com/junhaideng/sphincs/merkle 0.467s coverage: 68.2% of statements? github.com/junhaideng/sphincs/rand [no test files]ok github.com/junhaideng/sphincs/signature 1.367s coverage: 92.3% of statements 如果说我们想要单独看一个 package 中测试 case 所达到的覆盖度，可以指定包的名称 设置 -count=1 可以避免代码没修改导致的缓存，通俗来讲就是重新跑一遍 1go test github.com/junhaideng/sphincs/signature -cover -count=1 如果想要查看哪些具体行被覆盖了，可以设置 -coverprofile 参数， 1go test github.com/junhaideng/sphincs/signature -cover -count=1 -coverprofile=&quot;cover.out&quot; 然后将输出的文件转换成 html 文件 1go tool cover -html=&quot;cover.out&quot; -o cover.html 使用浏览器打开 html 文件，可以显示到那些代码行被覆盖了，哪些没有，这里的红色的表示测试没有覆盖到，绿色表示覆盖到了，灰色表示不用计算的行 提高代码的测试覆盖度有利于发现问题所在，但是一味的追求测试覆盖率会花费更多的时间，得不偿失。","link":"/2022/04/10/golang/fundamental/test/cover/"}],"tags":[{"name":"distribution","slug":"distribution","link":"/tags/distribution/"},{"name":"api","slug":"api","link":"/tags/api/"},{"name":"sort","slug":"sort","link":"/tags/sort/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"deploy","slug":"deploy","link":"/tags/deploy/"},{"name":"GitHub","slug":"GitHub","link":"/tags/GitHub/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"container","slug":"container","link":"/tags/container/"},{"name":"Go","slug":"Go","link":"/tags/Go/"},{"name":"Language","slug":"Language","link":"/tags/Language/"},{"name":"design pattern","slug":"design-pattern","link":"/tags/design-pattern/"},{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"proxy","slug":"proxy","link":"/tags/proxy/"},{"name":"Rust","slug":"Rust","link":"/tags/Rust/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"tools","slug":"tools","link":"/tags/tools/"},{"name":"GitHub Actions","slug":"GitHub-Actions","link":"/tags/GitHub-Actions/"},{"name":"Scheduler","slug":"Scheduler","link":"/tags/Scheduler/"},{"name":"Vim","slug":"Vim","link":"/tags/Vim/"},{"name":"hash","slug":"hash","link":"/tags/hash/"},{"name":"cryptography","slug":"cryptography","link":"/tags/cryptography/"},{"name":"signature","slug":"signature","link":"/tags/signature/"},{"name":"hash-based","slug":"hash-based","link":"/tags/hash-based/"},{"name":"OTS","slug":"OTS","link":"/tags/OTS/"},{"name":"FTS","slug":"FTS","link":"/tags/FTS/"},{"name":"stateless","slug":"stateless","link":"/tags/stateless/"},{"name":"design","slug":"design","link":"/tags/design/"},{"name":"source code","slug":"source-code","link":"/tags/source-code/"},{"name":"wireshark","slug":"wireshark","link":"/tags/wireshark/"},{"name":"tool","slug":"tool","link":"/tags/tool/"},{"name":"package","slug":"package","link":"/tags/package/"},{"name":"fundamental","slug":"fundamental","link":"/tags/fundamental/"},{"name":"Cargo","slug":"Cargo","link":"/tags/Cargo/"},{"name":"framework","slug":"framework","link":"/tags/framework/"},{"name":"Gin","slug":"Gin","link":"/tags/Gin/"},{"name":"testing","slug":"testing","link":"/tags/testing/"}],"categories":[{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"Golang","slug":"Golang","link":"/categories/Golang/"},{"name":"network","slug":"network","link":"/categories/network/"},{"name":"Rust","slug":"Rust","link":"/categories/Rust/"},{"name":"git","slug":"git","link":"/categories/git/"},{"name":"GitHub","slug":"GitHub","link":"/categories/GitHub/"},{"name":"tool","slug":"tool","link":"/categories/tool/"},{"name":"Cryptography","slug":"Cryptography","link":"/categories/Cryptography/"},{"name":"Note","slug":"Rust/Note","link":"/categories/Rust/Note/"},{"name":"Macro","slug":"Rust/Note/Macro","link":"/categories/Rust/Note/Macro/"}]}